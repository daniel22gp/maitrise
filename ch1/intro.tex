%!TEX root = ../memoire.tex

\chapter*{Introduction}

\pagenumbering{arabic}

% \noindent c'est mieux de changer la définition de \chapter et autres titres pour que ça soit uniforme

Les récentes percées en intelligence artificielle ne cessent de fasciner la population, notamment les applications liées au \acf{TAL} comme les systèmes de dialogue ordinateur-humain. Ainsi, de nombreuses recherches se sont orientées vers le développement de systèmes informatiques pouvant communiquer avec les humains. Or, la communication implique deux processus: la compréhension du langage et la production de celle-ci. Le deuxième processus a donné naissance à la \acf{GAT}, qui est une branche du \ac{TAL} dont le but est de produire du texte compréhensible en langue naturelle à partir de données non-linguistiques. Les premiers systèmes de \ac{GAT} ayant vu le jour ont été créés pour produire des rapports automatiquement dans le but d'alléger la charge de travail des humains \citep{ReiterBuildingNaturalLanguage2000}.

La \ac{GAT} s'avère donc extrêmement utile en termes d'applications concrètes, on s'en sert régulièrement pour produire des documents résumant des informations complexes pour des gens n'ayant pas les connaissances de bases requises pour les comprendre. Dans cette veine, le générateur MARQUIS \citep{WannerMARQUISGENERATIONUSERTAILORED2010} génère du texte à partir de données numériques brutes sur la qualité de l'air, ce qui permet à une grande population d'avoir accès à ces informations (en plusieurs langues). Toutefois, la \ac{GAT} ne permet pas que de résumer des informations complexes, elle sert aussi à combler un manque de couverture dans certains contextes. Par exemple, la \ac{GAT} permet la production d'articles sportifs dans des milieux où il n'y a pas de couverture médiatique, mais où il y a un besoin \citep{lareau11a, W17-3513}.

Traditionnellement, un système de \ac{GAT} se divise en deux parties. La première étant ce que \cite{DanlosPresentationmodelegeneration1983} appelle le \emph{quoi-dire} et que \cite{gatt18} appellent le \emph{early process}, qui équivaut à déterminer et structurer le contenu à transmettre. Autrement dit, il s'agit d'évaluer, parmi les données s'offrant à nous, lesquelles nous voulons transmettre au lecteur, et dans quel ordre. Ensuite il y a ce que \citeauthor{DanlosPresentationmodelegeneration1983} appelle le \emph{comment-le-dire} et que \citeauthor{gatt18} appellent le \emph{late process}, qui revient à choisir les unités lexicales qui serviront à véhiculer le \emph{quoi-dire} et à construire des phrases grammaticales avec ces unités. C'est ce qu'on appelle la \strong{réalisation linguistique}. Cette étape du processus de génération a fait l'objets de plusieurs travaux, entre autre parce qu'il existe diverses approches pour l'effectuer. Nous nous concentrerons sur l'une d'entre elles: la réalisation linguistique à base de règles, qui correspond à modéliser les connaissances linguistiques d'une langue donnée dans une grammaire et un dictionnaire.

Comme le lexique d'une langue est incommensurable, la plupart des réalisateurs n'ont pas accès à tous les mots d'une langue ainsi que toutes leurs propriétés de combinatoire. Toutefois, la majorité des parties du discours présentent des régularités quant à leurs comportements syntaxiques, à l'exception des verbes qui forment la partie du discours la plus capricieuse. Ainsi, si on souhaite qu'un réalisateur génère du texte le plus naturel possible, on devrait récupérer les propriétés des verbes parce qu'ils sont non seulement les moins prévisibles, puis ils contrôlent essentiellement la structure de tout énoncé. Cette problématique ne s'applique pas uniquement à la \ac{GAT}: toutes les branches du \ac{TAL} bénéficieraient de connaissances fines des verbes. \cite{Korhonenlargesubcategorizationlexicon2006, SchulerVerbnetBroadcoverageComprehensive2005} postulent qu'un meilleur traitement des langues naturelles passe par la connaissance des comportements syntaxiques des verbes. L'expression \scare{comportements syntaxiques} réfère aux arguments qu'une unité lexicale sélectionne. Par exemple, lorsqu'on dit qu'un verbe peut être à la fois transitif direct et intransitif, il s'agit là de deux comportements syntaxiques (intransitivité du verbe \lex{manger}: \form{Paul mange}, et transitivité: \form{Paul mange un dessert}). On encode généralement ces comportements dans des dictionnaires car on ne peut pas prédire les comportements des verbes à partir de règles grammaticales. \cite{MilicevicSchemaregimepont2009} illustre ce problème en montrant que deux verbes synonymiques montrent des comportements différents: \form{on se souvient de $X$}, mais \form{on se rappelle $X$}. L'obligation de la préposition dans la première construction est entièrement arbitraire, c'est pourquoi ce type d'information doit être encodé dans un dictionnaire pour qu'un système de \ac{GAT} sache comment réaliser ces constructions syntaxiques.

Ainsi, en détenant les propriétés syntaxiques des verbes d'une langue donnée, on arrive à couvrir une grande partie des constructions possibles pour cette langue. Nous pensons qu'un réalisateur profond bénéficierait d'une telle ressource, c'est pourquoi nous voulons intégrer à un réalisateur profond un dictionnaire de comportements syntaxiques renfermant une quantité exhaustive des constructions possibles de l'anglais. Nous avons choisi GenDR \citep{lareau18,lambrey15,LambreyGECOv1User2016,LambreyImplementationcollocationspour2017,dubinskaite17}, un réalisateur profond multilingue qui reprend les bases du système MARQUIS et qui opère dans le cadre théorique de la \ac{TST}.

La ressource lexicale que nous avons choisi pour améliorer la couverture de GenDR est VerbNet \citep{SchulerVerbnetBroadcoverageComprehensive2005}, un dictionnaire de verbes de l'anglais créé dans un contexte où il y avait un réel besoin de faire un dictionnaire décrivant la richesse et la complexité des verbes \citep{KipperClassBasedConstructionVerb2000}. \cite{SchulerVerbnetBroadcoverageComprehensive2005} trouvait qu'il y avait un manque de lignes directrices par rapport à l'organisation des verbes dans les dictionnaires destinés à des applications \ac{TAL}. Son dictionnaire est organisé en une hiérarchie de classes verbales héritées de \cite{verb-classes.levin.1993}, qui proposait une méthode de classification des verbes basée sur le partage des comportements syntaxiques entre ceux-ci. \citeauthor{verb-classes.levin.1993} a tenté de délimiter tous les patrons de régime possibles pour les verbes de la langue anglaise. Lorsque plusieurs présentaient des caractéristiques communes sur le plan syntaxique, elle rassemblait ces verbes dans une classe.

Pour implémenter VerbNet à GenDR, nous avons utilisé l'environnement Python qui nous a permi de manipuler et d'extraire les données de la ressource lexicale choisie. En effet, nous avons du les manipuler pour que les informations marchent avec la théorie Sens-Texte. Pour compléter l'implémentation de cette ressource, nous avons du revoir certaines règles de grammaire et l'architecture de nos dictionnaires pour tenir compte des nouvelles informations lexicales auxquelles nous aurons accès.

Nous avons séparé ce mémoire en six chapitres, dont les trois premiers correspondent à la préparation du projet et les trois derniers à l'exécution du projet. Ainsi, le premier chapitre décrit ce qu'est la \ac{GAT} en faisant un survol des six étapes du processus classique de génération de texte \citep{ReiterBuildingNaturalLanguage2000}. Nous retiendrons la dernière étape du processus, la réalisation, et nous la décrirons en détail. Après, nous présentons quelques réalisateurs pour montrer les différences théoriques et pratiques qui existent entre ceux-ci. Le deuxième chapitre porte entièrement sur le réalisateur profond GenDR. Nous décrivons comment ce réalisateur modélise les phénomènes langagiers et expliquons en détail les modules qui le composent. Le troisième chapitre est dédié à la description de diverses ressources lexicales potentielles, puis nous exposons pourquoi notre choix s'est arrêté sur GenDR et comment fonctionne cette ressource.

Dans le quatrième chapitre, nous expliquons comment nous utilisons Python pour préparer des scripts nous permettant d'extraire les données de VerbNet que nous désirions importer. Puis, le cinquième chapitre décrit comment nous avons adapté le réalisateur profond GenDR pour qu'il incorpore les données que nous avons extraites de VerbNet. Plus précisément, nous décrivons comment les règles de grammaire ont été adpatées, ainsi que les dictionnaires. Puis, le sixième chapitre comporte une évaluation du système basée sur le corpus de VerbNet. Enfin, la conclusion fait une synthèse de ce mémoire et évoque quelques pistes à explorer.