\chapter{Importation de VerbNet dans GenDR}\label{ch:python}

À l'aide du module \emph{xml.etree.cElementTree}\footnote{\url{https://docs.python.org/3/library/xml.etree.elementtree.html}, 01-06-17} créé pour le langage de programmation \emph{Python}, nous avons pu manipuler et extraire les données de VerbNet qui sont encodées en \emph{XML}. Ensuite, nous les avons compilées dans des fichiers\emph{.dict} pour les implémenter dans GenDR. Ainsi, nous créerons un dictionnaire de classes verbales, suivi d'un dictionnaire des membres de VerbNet et finalement, un dictionnaire de patron de régime. Les premiers dictionnaires seront fusionnées et intégrér à GenDR, puis le dictionnaire de patron de régime fonctionnera à part. De plus, nous avons aussi utilisé les données de VerbNet pour nous créer une banque de phrases exemples afin de tester notre système. L'extraction des exemples a aussi été effectuée dans l'environnement \emph{Python}.
 
\section{Adaptation du \emph{lexicon} de GenDR}

Le dictionnaire lexical que nous prévoyons utiliser pour GenDR se divisera en quatre sections: les classes abstraites (classes originaires de GenDR), les membres verbaux de VerbNet, les classes verbales et le reste du lexique. Pour adapter le \emph{lexicon}, il nous manque deux sections:\texttt{VERBNET CLASSES} et \texttt{VERBNET MEMBERS}. Nous décrirons dans les passages suivants comment les scripts Python que nous avons créé nous ont permi de bâtir ces futures section pour le \emph{lexicon} de la nouvelle version de GenDR.

D'abord, pour mieux comprendre la manière dont nous avons créé la section \texttt{VERBNET CLASSES}, il faut rappeler quelques notions de base du fonctionnement de GenDR. Plus tôt au chapitre \ref{chapgendr}, nous avons décrit le mécanisme d'héritage qui façonne l'architecture du lexique (section \ref{sec:dictio}). Ce mécanisme permet à une entrée d'hériter des traits d'une autre entrée. Concrètement, cela s'effectue en faisant pointer une entrée vers une autre (ex: \texttt{owe} : \texttt{verb\_dit}). Ainsi, nous n'avions pas à réécrire pour chaque verbe transitif, leur comportement syntaxique. En effet, on pouvait transmettre les propriétés du \ac{GP} des verbes transitifs à une entrée lexicale désirée. Nous avons donc importer l'architecture de VerbNet dans notre dictionnaire en ayant ce mécanisme en tête. Dans la version originale de VerbNet, les classes abstraites (ex: intransitif, transitif, ditransitif) incorporaient toute l'information syntaxique nécessaire: patrons de régime, diathèses, partie du discours, etc. Cela permettait, par exemple, à \lex{owe} d'en hériter via son association à l'une de ces classes.  

Nous reprendrons ce mécanisme pour deux objectifs. D'abord, pour reprendre l'architecture de VerbNet qui consiste à établir une hiérarchie entre les classes \scare{mères} et leurs classes \scare{filles}. Comme nous l'avons vu à la section \ref{sec:vnarchitecture}, les classes verbales sont hiérarchisées et les traits des classes dominantes sont transmis aux classes qu'elles gouvernent. Nous voulions donc reprendre cette architecture en créant la section \texttt{VERBNET CLASSES} avec le mécanisme d'héritage. Le but était de transmettre les patrons de régimes des classes \scare{mères} aux classes \scare{filles} en récupérant le mécanisme d'héritage. Puis, nous reprenons ce mécanisme pour transmettre les traits de partie du discours de la classe abstraite \texttt{VERB} afin d'éviter de répéter cette information pour chacune des classes verbales. Pour ce faire, on reprend le même concept qui lie les classes filles à leurs classes mères, mais cette fois-ci, on lie chaque classe mère à la classe abstraite VERB qui contient les \ac{SPOS} et \ac{DPOS} afin que les traits de partie du discours se transmette jusqu'à tous les lexèmes verbaux.

\begin{lstlisting}[language=XML, caption=Traits de la classe abstraite \texttt{VERB}]
verb {
  dpos = V
  spos = verb
}
\end{lstlisting}

Ensuite, nous avons extrait les verbes compris dans toutes les classes verbales de VerbNet et nous les avons soumis au mécanisme d'héritage. En effet, chaque membre pointe vers la classe ou la sous-classe qui le représente, ce qui fait en sorte qu'il hérite de tous les traits de la classe qui lui correspond. Par exemple, les lexèmes \lex{absorb}, \lex{ingest}, \lex{take in} hériteront tous les trois des traits compris dans l'entrée \texttt{absorb-39.8}. Cela nous permet de traiter 6\,393 acceptions sans avoir à décrire leur comportement syntaxique systématiquement.

\begin{lstlisting}[language=XML]
absorb: "absorb-39.8"
take_in: "absorb-39.8"
ingest_1: "absorb-39.8"
\end{lstlisting}

\subsection{Extractions de l'architecture de VerbNet}

Nous avons divisé la description du premier script en trois blocs pour faciliter la compréhension du fonctionnement de chacun de ceux-ci.

\subsubsection{Hiérarchie des classes verbales}

\textbf{L'objectif} de la fonction \emph{supers} est de recréer l'architecture de VerbNet pour pouvoir l'implémenter à GenDR. Cette fonction récupère l'identifiant de la classe \scare{mère}, puis de toutes les sous-classes imbriquées en elle. Ensuite, \textbf{la fonction} crée un dictionnaire dont la clé correspond à l'identifiant de la classe \scare{fille} et la valeur est l'identifiant de la classe \scare{mère}. Cela correspond au mécanisme qui nous permettra de transmettre les attributs d'une classe \scare{mère} à une classe \scare{fille}. Le résultat de cette fonction ressemble à \lstinline|"begin-55.1-1": "begin-55.1"|. Dans ce contexte, \texttt{begin-55.1-1} héritera des traits de la classe \texttt{begin-55.1}

\subsubsection{Création du dictionnaire de classes verbales}

\textbf{L'objectif} de la fonction \emph{treeframes} est de récupérer, pour chaque classe VerbNet, l'identifiant de la classe verbale, les identifiants des patrons de régime compris sous celle-ci et une phrase exemple. Ainsi, cette fonction rend un trio de ce type: \lstinline|{spray-9.7, NP_V_NP_destination, Jessica sprayed the wall}|.

La fonction \emph{treeframes} récupère d'abord l'identifiant de la classe \texttt{spray-9.7} puis, les identifiants des patrons de régime qui sont encodés dans la section \texttt{<FRAME>}. Ensuite, grâce aux expressions régulières, on manipule les identifiants de \ac{GP} pour qu'ils correspondent au type de code demandé par MATE. De plus, nous intégrons les prépositions régies par chaque patron de régime à l'intérieur de l'identifiant afin de les distinguer, puisque certains patrons de régime de VerbNet s'écrivent de la même manière, mais ne sélectionne pas les mêmes prépositions. Donc, lorsque nous encoderons les patrons de régime dans un dictionnaire, nous saurons quelle préposition inclure pour un patron de régime donné puisque ceux-ci se retrouveront dans l'identifiant que nous aurons repêché: \texttt{NP\_asset\_V\_NP\_PP\_from\_out\_of} est un identifiant de \ac{GP} dans lequel sont encodés les prépositions from et out of pour le troisième actant syntaxique.  

Finalement, on récupère les exemples accompagnant chaque patron de régime pour compléter le trio: identifiant de classe, de \acp{GP} et exemple. 

\subsubsection{Implémentation des données dans un fichier .dict}

L'objectif de ce dernier bloc de code est d'obtenir un dictionnaire qui contiendra les classes verbales de VerbNet avec tous les identifiants de patrons de régime que la classe englobe et les exemples qui correspondent à ceux-ci.

Pour procéder, nous ouvrons un fichier appellé \emph{lexicon.dict} dans lequel nous écrirons les informations que extrairons grâce aux deux fonctions que nous venons de créer. Nous fournirons ces fonctions à l'ensemble des documents XML qui composent la ressource VerbNet et le résultat de ce processus nous donne la section \texttt{VERBNET CLASSES} que nous implémenterons dans le dictionnaire de GenDR.

\begin{lstlisting}[language=Python, caption = Importation de l'architecture des classes verbales, label=fig:archivn]
# VERBNET HIERARCHY
def supers(t, i):
    ID = t.get('ID') # t = root of the verbal class, it contains the shared syntactic information.
    sc = {ID:i} # simulates the inheritance mechanism.
    subclasses = t.findall('SUBCLASSES/VNSUBCLASS') # gets all the information on the subclasses.
    if len(subclasses) > 0:
        for sub in subclasses:             # If there's a subclass for a given VNCLASS, 
            sc = {**sc, **supers(sub, ID)} # it'll point towards the class it's being dominated by.
    return sc
		
# EXTRACTION of GPS identification and EXAMPLES
def treeframes(t):
    ID = t.get('ID')  # gets the name of the verbnet class
    z = []            
    for frame in t.findall('FRAMES/FRAME'):
        description = re.sub(r"\s*[\s\.\-\ +\\\/\(\)]\s*", '_',  
        frame.find('DESCRIPTION').get('primary')) # primary description = identification of a GP
        if description in exclude:
            continue
        description = re.sub('PP', 'PP_{}', description) 
        preps = [p.get('value') or 
                p.find('SELRESTRS/SELRESTR').get('type').upper()                  
                for p in frame.findall('SYNTAX/PREP')+frame.findall('SYNTAX/LEX')] 
        preps = [sorted(p.split()) for p in preps] # manipulates data to insert the prep. in desc.                                
        examples = [e.text for e in frame.findall('EXAMPLES/EXAMPLE')] # get ex. for each desc.
        if len(preps)==1:
            description = description.format('_'.join(preps[0]))
        elif len(preps)==2:
            description = description.format('_'.join(preps[0]),
                                             '_'.join(preps[1]))
        elif len(preps)==3:
            description = description.format('_'.join(preps[0]), 
                                             '_'.join(preps[1]), 
                                             '_'.join(preps[2])) # inserting preps in descriptions
        z.append((description, examples))
        
    subclasses = t.findall('SUBCLASSES/VNSUBCLASS')  # gets the root of each subclasses
    subframes = [treeframes(subclass) for subclass in subclasses] # applies function to subclasses
    subframes = sum(subframes, []) # flatten list of lists
    return [(ID, z)] + subframes # returns list of (sub)class, GP-identification and example
		
# WRITING of the EXTRACTED INFORMATION in a DICT FILE
with open('lexicon.dict','w') as f: # we are going to write all of this block into lexicon.dict
    f.write('lexicon {\n')
    for file in [f for f in os.listdir('verbnet') if f[-4:] == '.xml']: # open VerbNet XMl files
        root = ET.parse('verbnet/'+file).getroot() # Applies the Python Element Tree module
        d = dict(treeframes(root)) # makes a dictionary out of the results of treeframes on a file
        sc = supers(root, 'verb') # applies supers function to each file
        for c in d.keys():
            f.write('"'+c+'"')
            if sc[c] == 'verb': # all non-dominated classes point to the default verb class
                f.write(': ' +sc[c] + ' {') 
            else:
                f.write(': ' +'"'+sc[c]+'"' + ' {') #dominated classes point towards their governor
            [f.write('\n  gp = { id=' + gp[0] + (max(len(gp[0]), 30)-len(gp[0]))
                     *' ' + ' dia=x } // ' + ' '.join(gp[1])) for gp in d[c]]
            f.write('\n}\n') # all GPs will have attributes: id and dia
    f.write('\n}')
\end{lstlisting}

\subsection{Création du dictionnaire des membres verbaux} \label{extracmembre}

Maintenant que nous avons extrait les classes verbales et l'architecture de celles-ci, il ne nous reste qu'à peupler le dictionnaire des lexèmes encodés dans la section \texttt{<MEMBERS>} de chaque classe de VerbNet. De cette manière notre \emph{lexicon} pourra effectivement couvrir une immense partie de l'anglais grâce au travail de %\cite{SchulerVerbnetBroadcoverageComprehensive2005}. À cette étape, nous procèderons aussi à la désambiguïsation des verbes. Schuler a distingué les différentes acceptions d'un même vocable en leurs assignant des classes verbales différents en fonction des différents sens d'un vocable.

\subsubsection{Récupérer les membres}
L'objectif de la fonction \emph{treemember} est de récupérer les membres assignés à chaque classe et sous-classe verbale encodés dans VerbNet.

Cette fonction récupère d'abord l'identifiant de la classe verbale puis les membres lui correspondant, et répète l'opération pour les sous-classes, puis la fonction retourne des paires de classes verbales et de membres. Par exemple, le script récupèrera deux fois le vocable, une fois de cette manière \lstinline| order : "get-13.5.1"|, puis, \lstinline| order : "order-60-1"|.

\subsubsection{Désambiguiser les membres}

L'objectif de ce bloc est de désambiguïser les différentes acceptions d'un même vocable. L'exemple que nous venons d'illuster pour le lexème \lex{order} démontre la nécessité de faire cette opération.

Pour remédier à la situation, nous avons extrait toutes les acceptions de mêmes vocables (qu'on a appelé des duplicats). Puis, nous les avons distingué en leur assignant un numéro (ex: \texttt{grill 1}, \texttt{grill\_2}, \texttt{grill\_3}). Finalement, nous avons identifié les vocables à une acception et nous les avons fusionné à la liste qui contenait tous les lexèmes désambiguïsés, ce qui résulte en l'entièreté des membres de VerbNet.

\subsubsection{Implémentation des membres dans un fichier .dict}

L'objectif de ce bloc est de créer le dictionnaire \emph{members} qui sera éventuellement intégré au \emph{lexicon} comme étant la section \texttt{VERBNET MEMBERS}.

Pour ce faire, nous classons d'abord les membres en ordre alphabétique, puis nous assignons, à chaque acception, la classe verbale qui lui correspond (ex: \lstinline|order_1 : "get-13.5.1"|).

\begin{lstlisting}[language=Python, caption = Ajout des membres de VerbNet, label=scriptmember]
#GET MEMBERS FROM VERBNET CLASSES
def treemember(t):
    ID = t.get('ID') # get ID of the VNCLASS
    members = [m.get('name') for m in t.findall('MEMBERS/MEMBER')] # get members 
    subclasses = t.findall('SUBCLASSES/VNSUBCLASS')
    submembers = []
    if len(subclasses) > 0: # if there's a subclass
        for sub in subclasses:
            submembers = submembers + treemember(sub) # get ID of the subclass and members
    return [(ID, members )] + submembers

# DISAMBIGUATE MEMBERS
files = [f for f in os.listdir('verbnet') if f[-4:] == '.xml']
members = dict(sum([treemember(ET.parse('verbnet/'+file).getroot())
 for file in files], [])) # VNCLASS : [member1, member2, etc. ]

values = sum(list(members.values()), []) # just the members of all classes

dups = {m:[ID for ID in members.keys() if m in members[ID]]
 for m in values if values.count(m)>1}  # get all the duplicates

lexemes = {d[0]+'_'+str(n+1):d[1][n]
 for d in dups.items() for n in range(len(d[1]))} # enumerate all duplicates: eat_1, eat_2

unique_member = {m:ID for ID in members.keys() 
 for m in values if m in members[ID] and values.count(m)==1} #  get all unique lexemes

unified_dict = {**unique_member, **lexemes} # fuse both dict. to get all members disambiguated

# WRITE MEMBERS IN A FILE
with open('members.dict','w') as f: # open a file
    f.write('members {\n')
    for key in sorted(unified_dict.keys()): # sort the members
        f.write(key) # write the members
        f.write(': '),
        f.write('"'+str(unified_dict[key])+'"') # point members towards ID of VNCLASS
        f.write('\n')
    f.write('\n}\n')
\end{lstlisting}

\section{Création du dictionnaire de patrons de régime}

Lors de la création de la section \texttt{VERBNET CLASSES}, nous avons extrait les classes verbales de VerbNet ainsi que les identifiants des patrons de régime qui sont encodés dans ces classes. Nous avons analysé le contenu des patrons de régime prélevés afin de trouver une manière d'encoder les propriétés lexicales de chaque patron de régime dans un dictionnaire prêt à cet effet.

L'analyse du contenu des patrons de régime nous a fait remarqué qu'il y avait certains comportements syntaxiques encodés dans les classes verbales dont nous voulions nous débarrasser. Il s'agissait en quelque sorte de faire un tri initial des \acp{GP} qui pourraient poser problèmes et des \acp{GP} que nous jugions inutiles. 

Nous avons d'abord exclu les constructions stylistiques comme \lstinline|NP_location_V_NP| qui réalise ce type de phrase \form{All through the mountains raged a fire.}. Puis, nous avons aussi exclu les \acp{GP} qui sélectionnent des constructions de type \emph{déplacement qu-}, car GenDR ne traite pas ces comportements pour l'instant. Il a aussi fallu enlever les constructions contenant des modificateurs de types adverbiaux ou adjectivaux, par exemple \lstinline|NP_V_ADVP_Middle_PP_into_to_with| qui s'exemplifie par la phrase \form{The computer connected \textbf{well} to the network.}. L'emploi d'un groupe adverbial dans un patron de régime n'a pas sa place selon nous, car il ne s'agit pas d' un actant syntaxique et il n'est certainement pas sélectionné par le verbe. Au contraire, c'est l'adverbe qui le modifie.

\draft{refaire ce paragraphe}Après avoir fait filtrer le tout, nous avons donc procédé à la création du dictionnaire de patron de régime. Nous en sommes venus à la conclusion que nous ne pouvions pas directement extraire les propriétés lexicales des \acp{GP} de VerbNet, bien que c'était l'objectif initial. Comme nous l'avons vu plus tôt, un patron de régime englobe (diathèse, combinatoire lexicale et syntaxique d'une lexie (dpos, prep et rel)), mais les cadres syntaxiques de VerbNet ne nous donnait pas complètement ce X (pas diathèse, mais rôles théamtiques, la dpos (NP pour dpos=N ou S\_INF/S\_ING pour dpos=V), les prépositions et les relations via les phrases exemples (implicitement)). Bref, on avait tout ce dont on avait besoin avec les identifiants de GP extraits pairé avec les phrases exemples. Cependant, les rôles thématiques ne remplacent en aucun cas la diathèse, c'était un morceau qui aura fallu encoder manuellement.

Grâce à ces informations, qui sont encodées dans la section \texttt{<FRAMES>} de VerbNet, mais que nous avons extraits et encodé, nous avons pu créer de toutes pièces un dictionnaire de patron de régime (\emph{gpcon}).

Pour mieux comprendre le code qui nous a permi de créer le \emph{gpcon}, nous décrirons brièvement les étapes importantes qui ont mené à sa complétion. Donc, on prenait la description qui nous donnait les: dpos, relation, et préposition lié à un actant. Puis on créait une entrée de gp à partir de ça. À l'aide d'une fonction, nous avons pu parcourir toutes les calsses verbales de VerbNet et trouver qu'il y avait 274 identifiants de GP unique. Nous savions donc combien de gp il nous fallait coder. Pour procédéer à la chose, nous avons donc pris chaque gp unique et en avons fait une entrée dans notre dictionnaire. Nous l'avons encodé manuellement puisqu'aucune partie de VerbNet ne nous permettait de l'importer directement puisque le cadre théorique dans lequel nous nous insérons et GenDR demande que les patrons de régime soient décrits d'une certaine façon. De plus, comme ni la diathèse, ni relations de surface sont explicités, il fallait le faire nous-même. Pour accélérer le processus nous avons défini des objets qui représentent des actants syntaxiques non-étiquettés, mais qui incorprorent les traits lexicaux qu'un tel actant syntaxique demanderait.

{définir les objets, définir une fonction pour prendre les objets et leur attribuer une étiquette syntaxique, puis créer les régimes grâce à ça}

\subsection{Propriétés des actants syntaxiques}

Dans le premier bloc, nous définissons des objets comme subj ou dir\_N qui correspondent à des propriétés syntaxiques qu'on attribue normalement à un actant syntaxique. Par exemple, l'objet subj contient les traits suivants \lstinline|subj = 'rel=subjective dpos=N'|, qui sont généralement attribués au premier actant syntaxique. Nous avons ainsi défini toutes les propriétés syntaxiques possibles des actants que nous avons relevé dans les \acp{GP} de VerbNet. Ainsi, ces objets correspondent à des raccourcis pour faciliter l'encodage des \acp{GP} individuels. Puisque nous avons codé chaque patron de régime manuellement, il nous fallait un raccourci pour décrire les propriétés d'un actant syntaxique. Par exemple, au lieu d'écrire au complet \lstinline|I = { rel = subjective dpos = N}| à chaque fois qu'un patron de régime a un tel actant syntaxique, nous n'avons qu'à spécifier l'objet défini dans cette section. 

Cela nous a permi de spécifier l'objet dans la description du régime au lieu de toutes les propriétés de l'actant syntaxique à chaque fois. Cela a grandement augmenté l'efficacité de l'encodage des 274 patrons de régime. Par exemple, \lstinline|NP_agent_V_NP| est décrit par les éléments: [subj, dir\_N] qui sont respectivement encodé en fonction de leur relation syntaxique de surface. Donc le système vérifie que contient l'item \texttt{subj} puis récupère les valeurs à l'intérieur \lstinline|subj = 'rel=subjective dpos=N'| et il répète l'opération jusqu'à ce que toutes les propriétés syntaxiques soient récupérées.

\subsection{Assignation des étiquettes syntaxiques profondes aux actants}

L'objectif de la combinaison des fonctions \emph{roman} et \emph{gp} est d'assigner les étiquettes des actants syntaxiques selon leur emplacement dans la description d'un \ac{GP}. Par exemple, dans \lstinline|NP_agent_V_NP: [subj, dir_N]|, puisque \texttt{dir\_N} est à la deuxième position, les propriétés qu'il renferme seront encodés dans un actant syntaxique \texttt{II}. En effet, si on se fie au produit final, nous avons dans le dictionnaire de patron de régime: \lstinline|NP_agent_V_NP { I={rel=subjective dpos=N} II={rel=dir_objective dpos=N} }|.  Cette fonction nous permet aussi de créer, par exemple, deux fois le même actant syntaxique pour un même \ac{GP}, afin de tenir compte du fait que certains actants sélectionnent deux prépositions: \lstinline| III={rel=oblique dpos=N prep=from}| et \lstinline|III={rel=oblique dpos=N prep="out of"}|. 

Ce qui est illustré par le \ac{GP} \lstinline|NP_asset_V_NP_PP_from_out_of| qui se décrit par les items suivants: \lstinline|[subj, dir_N, [from_N, out_of_N]]|

\subsubsection{Construction d'une entrée de \ac{GP}}


\subsubsection{Création du gpcon}
Finalement, nous créons le dictionnaire \emph{gpcon} en imposant la fonction \emph{gp} à toutes les descriptions de \acp{GP} pour que l'actant syntaxique (I,II,etc.) soit réalisé et pour qu'il renferme les propriétés syntaxiques qui lui sont associées.

\begin{lstlisting}[language=Python, caption = code pour gpcon.dict]

# 1 SYNTACTIC ACTANTS PROPERTIES
#subjective
subj = 'rel=subjective dpos=N'

#direct obj
dir_N = 'rel=dir_objective dpos=N'
dir_V_ING = 'rel=dir_objective dpos=V finiteness=GER'
dir_V_INF = 'rel=dir_objective dpos=V finiteness=INF'

#indirect obj
to_N = 'rel=indir_objective dpos=N prep=to'
indir_N = 'rel = indir_objective dpos = N'

#oblic
on_V = 'rel=oblique dpos=V prep=on'
to_obl_N = 'rel=oblique dpos=N prep=to' 
for_obl_N = 'rel=oblique dpos=N prep=for'
against_N = 'rel=oblique dpos=N prep=against'
...

# 2 ASSIGN SYNTACTIC LABEL to ACTANT
def roman(n):
    return ['I', 'II', 'III', 'IV', 'V', 'VI'][n-1] # transforms arabic numbers in roman numbers
def gp(name, real_actant):
    s = name + ' {\n'
    i=0
    for actant in real_actant:
        i = i+1                   # starts to enumerate at 1
        if type(actant) == list:  # if not actant but list, apply function to actants in list
            for y in actant:
                s = s + "   " + roman(i) + "={" + y + "}\n"
        else:
            s = s + "   " + roman(i) + "={" + actant + "}\n" # apply function to actant
    s = s + '}\n'
    return s 

# 3 CONSTRUCTION OF A GP ENTRY
descriptions = {
'NP_agent_V': [subj],
'NP_agent_V_NP': [subj, dir_N],
'NP_asset_V_NP_PP_from_out_of': [subj, dir_N, [from_N, out_of_N]],
...

# 4 GPCON CREATION
with open('gpcon.dict','w') as f: 
    f.write('gpcon {\n')
    for d in descriptions.keys():       # for each gps descriptions,
        f.write(gp(d, descriptions[d])) # write them with the correct syntactic label
    f.write('}')
\end{lstlisting}

\section{Scripts pour l'évaluation de GenDR}

Pour évaluer la qualité de l'implémentation de VerbNet à GenDR, nous avons extrait les phrases exemples qui figurent dans VerbNet dans le but d'en faire des inputs et de tenter de les générer. Autrement dit, nous avons pris les phrases exemples accompagnant chaque cadre syntaxique, et nous voulions créer des graphes sémantiques à partir de ces phrases, dans le but de les donner à GenDR pour qu'il tente de générer la phrase de départ.

\subsection{Extraction des exemples}

L'\textbf{extraction des exemples} se fait grâce à la fonction \emph{treeframe} que nous reprenons du script ayant servi à extraire les identifiants de \acp{GP} (voir figure~\ref{fig:archivn}). L'objectif est de parcourir les fichiers XML pour uniquement récupérer les phrases exemples contenues dans les classes verbales et sous-classes.

Le second bloc de code s'occupe de la \textbf{compilation des phrases} extraites dans le fichier \emph{phrases.txt}, en les divisant ligne par ligne. 

\begin{lstlisting}[language=Python, caption = Extraction des phrases exemples de VerbNet]
#EXAMPLES EXTRACTION
def treeframes(t):
    z = []
    for frame in t.findall('FRAMES/FRAME'): # for each syntactic frame
        description = re.sub(r"\s*[\s\.\-\ +\\\/\(\)]\s*", '_',
				frame.find('DESCRIPTION').get('primary'))
        if description in exclude:
            continue    
        examples = [e.text for e in frame.findall('EXAMPLES/EXAMPLE')] # get the examples
        z =  z + examples 
    subclasses = t.findall('SUBCLASSES/VNSUBCLASS')
    subframes = [treeframes(subclass) for subclass in subclasses] #repeat operation for subclasses
    subframes = sum(subframes, []) # flatten list of lists
    return z + subframes

#LIST OF SENTENCES IN THE FILE phrases.txt
liste=[]
with open('phrases.txt','w') as f:
    for file in [f for f in os.listdir('verbnet') if f[-4:] == '.xml']:
        root = ET.parse('verbnet/'+file).getroot()       
        d = (treeframes(root))  # Applies treeframes function to all of VerbNet files
        finale_liste = liste + d
        [f.write(x+'\n') for x in finale_liste] # returns line after each example

\end{lstlisting}

\subsection{Création des structures sémantiques}\label{sec:pythonstruc}

Ce script crée les graphes de bases qui nous permettront de faire les tests. Ces inputs en préparation sont dépourvus de n\oe{}uds et d'arcs. Ils ne contiennent que des éléments non-sémantiques qui faciliteront l'encodage des inputs.

Pour ce faire, nous ouvrons le fichier \emph{phrases.txt} qui contient chaque phrase exemple, ligne par ligne. Le script créera 978 structures sémantiques vides pour les 978 phrases extraites de VerbNet. Il identifiera chaque structure de 001 à 978. Les stuctures créées par ce script ne contiendront que le texte à représenter sémantiquement et les crochets \{ \} nécessaires pour encadrer le graphe. Finalement, le script créera 978 structures.

\begin{lstlisting}[language=Python, caption = Code pour créer les structures sémantiques vides, label=structurepython]
phrases = open('phrases.txt','r')

with open('structures.str','w') as f: # create a .str structure
    for(i,p) in enumerate(phrases):   # for each sentence
        with open('s'+str(i)+'.str','w') as g:
            structure = 'structure Sem S'+str(i)+' # name each structure by enumeration
						{\n S {text="'+p.strip()+'"\n\n main-> \n }\n}' # insert as texte the sentence
            f.write(structure)
            g.write(structure)
\end{lstlisting}

