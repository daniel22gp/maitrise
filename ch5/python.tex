\chapter{Importation de VerbNet dans GenDR}\label{ch:python}

À l'aide du module \emph{xml.etree.cElementTree}\footnote{\url{https://docs.python.org/3/library/xml.etree.elementtree.html}, 01-06-17} créé pour le langage de programmation \emph{Python}, nous avons pu manipuler et extraire les données de VerbNet qui sont encodées en \emph{XML}. Ensuite, nous les avons compilées dans des fichiers\emph{.dict} pour les implémenter dans GenDR. Ainsi, nous créerons un dictionnaire de classes verbales, suivi d'un dictionnaire des membres de VerbNet et finalement, un dictionnaire de patron de régime. Les premiers dictionnaires seront fusionnées et intégrér à GenDR, puis le dictionnaire de patron de régime fonctionnera à part. De plus, nous avons aussi utilisé les données de VerbNet pour nous créer une banque de phrases exemples afin de tester notre système. L'extraction des exemples a aussi été effectuée dans l'environnement \emph{Python}.
 
\section{Préparation du nouveau \emph{lexicon} de GenDR}

Le dictionnaire lexical que nous prévoyons utiliser pour GenDR se divisera en quatre sections: les classes abstraites (classes originaires de GenDR), les membres verbaux de VerbNet, les classes verbales et le reste du lexique. Pour adapter le \emph{lexicon}, il nous manque deux sections:\texttt{VERBNET CLASSES} et \texttt{VERBNET MEMBERS}. Nous décrirons dans les passages suivants comment les scripts Python que nous avons créé nous ont permi de bâtir ces futures section pour le \emph{lexicon} de la nouvelle version de GenDR.

D'abord, pour mieux comprendre la manière dont nous avons créé la section \texttt{VERBNET CLASSES}, il faut rappeler quelques notions de base du fonctionnement de GenDR. Plus tôt au chapitre \ref{chapgendr}, nous avons décrit le mécanisme d'héritage qui façonne l'architecture du lexique (section \ref{sec:dictio}). Ce mécanisme permet à une entrée d'hériter des traits (patrons de régime, diathèses, partie du discours, etc.) d'une classe abstraite. Cette transmission s'effectuait en faisant pointer une entrée vers une classe abstraite (ex: \texttt{owe} : \texttt{verb\_dit}). Ainsi, nous n'avions pas à réécrire pour chaque verbe transitif, leur comportement syntaxique. Nous avons donc pensé l'importation des classes verbales de VerbNet et de leurs membres en ayant ce mécanisme en tête.  

\draft{relire}Nous reprendrons ce mécanisme pour deux objectifs. D'abord, pour reprendre l'architecture de VerbNet qui consiste à établir une hiérarchie entre les classes \scare{mères} et leurs classes \scare{filles}. Comme nous l'avons vu à la section \ref{sec:vnarchitecture}, les classes verbales sont hiérarchisées et les traits des classes dominantes sont transmis aux classes qu'elles gouvernent. Nous voulions donc reprendre cette architecture pour la section \texttt{VERBNET CLASSES}. Le but était de transmettre les patrons de régimes des classes \scare{mères} aux classes \scare{filles} grâce au mécanisme d'héritage. Puis, nous reprenons ce mécanisme pour transmettre les traits de partie du discours de la classe abstraite \texttt{VERB} afin d'éviter de répéter cette information pour chacune des classes verbales. Pour ce faire, on reprend le même concept qui lie les classes \scare{filles} à leurs classes \scare{mères}, mais cette fois-ci, on lie chaque classe mère à la classe abstraite \texttt{VERB} qui contient les traits de partie du discours afin qu'ils se transmettent à tous les lexèmes verbaux.

\begin{lstlisting}[language=mate, caption=Traits de la classe abstraite \texttt{VERB}]
verb {
  dpos = V
  spos = verb
}
\end{lstlisting}

Ensuite, nous avons extrait les verbes (membres) compris dans toutes les classes verbales de VerbNet et nous les avons aussi soumise à ce mécansime. En effet, chaque membre pointera vers la classe ou la sous-classe qui le représente, ce qui fait en sorte qu'il héritera de tous les traits de la classe qui lui correspond. Par exemple, les lexèmes \lex{absorb}, \lex{ingest} et \lex{take in} hériteront, tous les trois, des traits compris dans la classe verbale \texttt{absorb-39.8}. Cela nous permet de traiter 6\,393 acceptions sans avoir à décrire leur comportement syntaxique systématiquement, ce qui évite de saturer notre dictionnaire lexical.

\begin{lstlisting}[language=mate, caption=Membres verbaux pointant vers leur classe correspondante]
absorb: "absorb-39.8"
take_in: "absorb-39.8"
ingest_1: "absorb-39.8"
\end{lstlisting}

\subsection{Extractions de l'architecture de VerbNet}

Nous avons divisé la description du premier script en trois blocs pour en faciliter la compréhension.

\subsubsection{Bloc 1: Hiérarchie des classes verbales}

\textbf{L'objectif} de la fonction \emph{supers} est de recréer l'architecture de VerbNet pour pouvoir l'implémenter à GenDR. Cette fonction récupère l'identifiant de la classe \scare{mère}, puis de toutes les sous-classes imbriquées sous elle. Ensuite, \textbf{la fonction} crée un dictionnaire \emph{Python} de type \emph{key: value} dont la clé correspond à l'identifiant de la classe \scare{fille} et la valeur est l'identifiant de la classe \scare{mère}. Cette configuration correspond à celle qui est nécessaire à GenDR pour l'application du mécanisme qui nous permettra de transmettre les attributs d'une classe \scare{mère} à une classe \scare{fille}. Le résultat de cette fonction ressemble à: \lstinline|"begin-55.1-1": "begin-55.1"|. Dans ce contexte, \texttt{begin-55.1-1} héritera des traits de la classe \texttt{begin-55.1}.

\subsubsection{Bloc 2: Création du dictionnaire de classes verbales}

\textbf{L'objectif} de la fonction \emph{treeframes} est de récupérer, pour chaque classe VerbNet, l'identifiant de la classe verbale, les identifiants des patrons de régime compris sous celle-ci et une phrase exemple. Ainsi, cette fonction rend un trio: \lstinline|{spray-9.7, NP_V_NP_destination, Jessica sprayed the wall}|.

Plus précisément, la fonction \emph{treeframes} récupère d'abord l'identifiant de la classe \texttt{spray-9.7} puis, les identifiants des patrons de régime qui sont encodés dans la section \texttt{<FRAME>} de cette classe verbale. Ensuite, grâce aux expressions régulières, on manipule les identifiants de \ac{GP} pour qu'ils correspondent au type de code demandé par GenDR. De plus, nous intégrons les prépositions régies par chaque patron de régime à l'intérieur de l'identifiant afin de les distinguer, puisque certains patrons de régime de VerbNet s'écrivent de la même manière, mais ne sélectionne pas les mêmes prépositions. Cette distinction s'avèrera très utile pour la description des patrons de régime dans le \emph{gpcon}, section \ref{sec:creategpcon}. Finalement, on récupère les exemples accompagnant chaque patron de régime pour compléter le trio: identifiant de classe, identifiant de \acp{GP} et exemple. 

\subsubsection{Bloc 3: Implémentation des données dans le fichier \emph{lexicon.dict}}

\textbf{L'objectif} de ce dernier bloc de code est de créer un fichier qui contiendra le résultat des deux fonctions sur l'ensemble des données de VerbNet. Pour procéder, nous ouvrons un fichier appellé \emph{lexicon.dict} dans lequel nous écrirons les informations de VerbNet concernant les cadres syntaxiques et les classes verbales de VerbNet manipulés grâces aux fonctions \emph{treeframes} et \emph{supers}. Le produit de ce bloc correspondra à la section \texttt{VERBNET CLASSES} du prochain \emph{lexicon} de GenDR.

\begin{lstlisting}[language=Python, caption = Importation de l'architecture des classes verbales, label=fig:archivn]
# BLOCK 1 : VERBNET HIERARCHY
def supers(t, i):
    ID = t.get('ID') # t = root of the verbal class, it contains the shared syntactic information.
    sc = {ID:i} # simulates the inheritance mechanism.
    subclasses = t.findall('SUBCLASSES/VNSUBCLASS') # gets all the information on the subclasses.
    if len(subclasses) > 0:
        for sub in subclasses:             # If there's a subclass for a given VNCLASS, 
            sc = {**sc, **supers(sub, ID)} # it'll point towards the class it's being dominated by.
    return sc
		
# BLOCK 2 : EXTRACTION of GPS identification and EXAMPLES
def treeframes(t):
    ID = t.get('ID')  # gets the name of the verbnet class
    z = []            
    for frame in t.findall('FRAMES/FRAME'):
        description = re.sub(r"\s*[\s\.\-\ +\\\/\(\)]\s*", '_',  
        frame.find('DESCRIPTION').get('primary')) # primary description = identification of a GP
        if description in exclude:
            continue
        description = re.sub('PP', 'PP_{}', description) 
        preps = [p.get('value') or 
                p.find('SELRESTRS/SELRESTR').get('type').upper()                  
                for p in frame.findall('SYNTAX/PREP')+frame.findall('SYNTAX/LEX')] 
        preps = [sorted(p.split()) for p in preps] # manipulates data to insert the prep. in desc.                                
        examples = [e.text for e in frame.findall('EXAMPLES/EXAMPLE')] # get ex. for each desc.
        if len(preps)==1:
            description = description.format('_'.join(preps[0]))
        elif len(preps)==2:
            description = description.format('_'.join(preps[0]),
                                             '_'.join(preps[1]))
        elif len(preps)==3:
            description = description.format('_'.join(preps[0]), 
                                             '_'.join(preps[1]), 
                                             '_'.join(preps[2])) # inserting preps in descriptions
        z.append((description, examples))
        
    subclasses = t.findall('SUBCLASSES/VNSUBCLASS')  # gets the root of each subclasses
    subframes = [treeframes(subclass) for subclass in subclasses] # applies function to subclasses
    subframes = sum(subframes, []) # flatten list of lists
    return [(ID, z)] + subframes # returns list of (sub)class, GP-identification and example
		
# BLOCK 3 : WRITING of the EXTRACTED INFORMATION in a FILE
with open('lexicon.dict','w') as f: # we are going to write all of this block into lexicon.dict
    f.write('lexicon {\n')
    for file in [f for f in os.listdir('verbnet') if f[-4:] == '.xml']: # open VerbNet XMl files
        root = ET.parse('verbnet/'+file).getroot() # Applies the Python Element Tree module
        d = dict(treeframes(root)) # makes a dictionary out of the results of treeframes on a file
        sc = supers(root, 'verb') # applies supers function to each file
        for c in d.keys():
            f.write('"'+c+'"')
            if sc[c] == 'verb': # all non-dominated classes point to the default verb class
                f.write(': ' +sc[c] + ' {') 
            else:
                f.write(': ' +'"'+sc[c]+'"' + ' {') #dominated classes point towards their governor
            [f.write('\n  gp = { id=' + gp[0] + (max(len(gp[0]), 30)-len(gp[0]))
                     *' ' + ' dia=x } // ' + ' '.join(gp[1])) for gp in d[c]]
            f.write('\n}\n') # all GPs will have attributes: id and dia
    f.write('\n}')
\end{lstlisting}

\subsection{Création du dictionnaire des membres verbaux} \label{extracmembre}

Maintenant que nous avons complété la section \texttt{VERBNET CLASSES} du prochain dictionnaire de GenDR, il ne nous reste qu'à peupler le dictionnaire des verbes que VerbNet a rassemblé. De cette manière notre \emph{lexicon} pourra effectivement couvrir une immense partie de l'anglais grâce au travail de \cite{SchulerVerbnetBroadcoverageComprehensive2005} qui a associé plus de 6\,000 verbes de l'anglais aux classes verbales. Lors de l'importation de ces verbes, nous procèderons aussi à leur désambiguïsation. Bien que Schuler ait distingué les différentes acceptions d'un même vocable en leurs assignant des classes verbales différentes, la forme des verbes n'est pas désambiguïsée. Pour que GenDR interprète correctement chaque entrée verbale, ils doivent porter des étiquettes différentes. Nous avons donc prévu un script à cet effet.

\subsubsection{Récupérer les membres}
L'objectif de la fonction \emph{treemember} est de récupérer les membres assignés à chaque classe et sous-classe verbale encodés dans VerbNet. Cette fonction récupère d'abord l'identifiant de la classe verbale, puis les membres qui lui correspondent. Finalement, la fonction retourne des paires de classes verbales et de membres. Par exemple, le script récupèrera deux fois le vocable \lex{order}, une fois de cette manière \lstinline|{order : "get-13.5.1"}|, puis, \lstinline|{order : "order-60-1"}|. À cette étape, nous venons de récupérer la désambiguïsation de VerbNet, mais elle ne s'insère pas dans notre dictionnaire puisque nous avons besoin que les formes entre ces acceptions diffèrent.

\subsubsection{Désambiguïser les membres}

L'objectif de ce bloc est de désambiguïser les différentes acceptions d'un même vocable. L'exemple que nous venons d'illuster pour le lexème \lex{order} démontre la nécessité de faire cette opération.

Pour remédier à la situation, nous avons extrait toutes les acceptions de mêmes vocables (qu'on a appelé des duplicats). Puis, nous les avons distingué en leur assignant un numéro (ex: \texttt{grill\_1}, \texttt{grill\_2}, \texttt{grill\_3}). Finalement, nous avons identifié les vocables à une acception et nous les avons mis à part dans une liste. Nous avons ensuite fusionné les acceptions désambiguïsées et celles qui ne le nécessitaient pas dans une même liste.

\subsubsection{Implémentation des membres dans un fichier .dict}

L'objectif de ce bloc est de créer le dictionnaire \emph{members.dict} qui sera éventuellement intégré au \emph{lexicon.dict} comme étant la section \texttt{VERBNET MEMBERS}.Pour ce faire, nous classons alphabétiquement les membres, puis nous assignons à chaque acception la classe verbale qui lui correspond (ex: \lstinline|order_1 : "get-13.5.1"|) grâce à la fonction \emph{treemember}. Finalement, nous insérons le tout dans le fichier \emph{members.dict}



\begin{lstlisting}[language=Python, caption = Ajout des membres de VerbNet, label=scriptmember]
#GET MEMBERS FROM VERBNET CLASSES
def treemember(t):
    ID = t.get('ID') # get ID of the VNCLASS
    members = [m.get('name') for m in t.findall('MEMBERS/MEMBER')] # get members 
    subclasses = t.findall('SUBCLASSES/VNSUBCLASS')
    submembers = []
    if len(subclasses) > 0: # if there's a subclass
        for sub in subclasses:
            submembers = submembers + treemember(sub) # get ID of the subclass and members
    return [(ID, members )] + submembers

# DISAMBIGUATE MEMBERS
files = [f for f in os.listdir('verbnet') if f[-4:] == '.xml']
members = dict(sum([treemember(ET.parse('verbnet/'+file).getroot())
 for file in files], [])) # VNCLASS : [member1, member2, etc. ]

values = sum(list(members.values()), []) # just the members of all classes

dups = {m:[ID for ID in members.keys() if m in members[ID]]
 for m in values if values.count(m)>1}  # get all the duplicates

lexemes = {d[0]+'_'+str(n+1):d[1][n]
 for d in dups.items() for n in range(len(d[1]))} # enumerate all duplicates: eat_1, eat_2

unique_member = {m:ID for ID in members.keys() 
 for m in values if m in members[ID] and values.count(m)==1} #  get all unique lexemes

unified_dict = {**unique_member, **lexemes} # fuse both dict. to get all members disambiguated

# WRITE MEMBERS IN A FILE
with open('members.dict','w') as f: # open a file
    f.write('members {\n')
    for key in sorted(unified_dict.keys()): # sort the members
        f.write(key) # write the members
        f.write(': '),
        f.write('"'+str(unified_dict[key])+'"') # point members towards ID of VNCLASS
        f.write('\n')
    f.write('\n}\n')
\end{lstlisting}

\section{Création du dictionnaire de patrons de régime}\label{sec:creategpcon}

Lors de la création de la section \texttt{VERBNET CLASSES}, nous avons extrait les classes verbales de VerbNet ainsi que les identifiants des patrons de régime qui y sont encodés. Nous avons ensuite analysé le contenu des patrons de régime prélevés afin de trouver la meilleure manière d'encoder les propriétés lexicales et syntaxiques de chaque patron de régime dans un dictionnaire.

D'abord, l'analyse du contenu \acp{GP} nous a fait remarqué qu'il nous fallait se débarrasser de certaines constructions syntaxiques. Nous avons ainsi filtrer \acp{GP} qu'on voulait importer dans un dictionnaire afin de départir ceux qui pourraient poser problèmes et ceux que nous jugions inutiles. 

Nous avons exclu les constructions stylistiques comme \lstinline|NP_location_V_NP| qui réalise ce type de phrase \form{All through the mountains raged a fire.}. Puis, nous avons aussi exclu les \acp{GP} qui sélectionnent des constructions de type \emph{déplacement qu-}, car GenDR ne traite pas ces comportements pour l'instant. Il a aussi fallu enlever les constructions contenant des modificateurs de types adverbiaux ou adjectivaux, par exemple \lstinline|NP_V_ADVP_Middle_PP_into_to_with| qui s'exemplifie par la phrase \form{The computer connected \textbf{well} to the network.}. L'emploi d'un groupe adverbial dans un patron de régime n'a pas sa place selon nous, car il ne s'agit pas d' un actant syntaxique et il n'est certainement pas sélectionné par le verbe. Au contraire, c'est l'adverbe qui le modifie. Environ une centaine de constructions syntaxiques ont été retirées pour ce type de raisons. Bref, après avoir fait filtré le tout, nous avons donc procédé à la création du dictionnaire de patron de régime. 

Nous en sommes venus à la conclusion que nous ne pouvions pas directement extraire toutes les propriétés lexicales des \acp{GP} de VerbNet, bien que c'était l'objectif initial. Comme nous l'avons vu plus tôt, un patron de régime englobe: la diathèse et la combinatoire lexicale et syntaxique d'une lexie (sa partie du discours, sa relation de surface et les prépositions régies), mais les cadres syntaxiques de VerbNet ne nous donnaient pas complètement ces informations. La nature des actants syntaxique est évidemment absente des cadres syntaxiques de VerbNet, puisque cette ressource utilise les rôles thématiques pour distinguer les actants liés au verbe. Toutefois, nous pouvons accéder à la partie du discours qui est encodée ainsi: \texttt{NP} correspond à \texttt{dpos=N} et \texttt{S\_INF/S\_ING} pour les verbes qui sélectionnent d'autres verbes correspond à \texttt{dpos=V}. VerbNet nous donne aussi accès à l'encodage aux prépositions régies qui sont encodées dans les cadres syntaxiques. Toutefois, les relations de surface ne sont pas explicités directement dans les cadres syntaxiques, mais nous pouvons les déduire grâce aux phrases exemples accompagnant les identifiants de patrons de régime que nous avons prélevé plus tôt.

Bref, nous avons manipuler l'extraction des identifiants de \acp{GP} pour que ceux-ci nous donnent le plus d'information possible. Ainsi, un identifiant comme : \texttt{NP\_V\_NP\_destination\_PP\_with\_theme} nous donne les informations suivantes: la partie du discours de chaque actant syntaxique, l'ordre des actants syntaxique en \ac{RSyntP} et les prépositions régies par le verbe pour un actant donné. Ensuite, l'union de cet l'identifiant à la phrase \form{Jessica loaded the wagon with boxes.} nous permet de déduire les relations de surface (sujet, objet direct, etc.) de chaque actant syntaxique. C'est grâce à ces informations que nous avons construit le \emph{gpcon}.

À l'aide d'un petit script Python, nous avons pu établir qu'il y avait 274 identifiants de \ac{GP} uniques. Ceux-ci correspondent à la liste de \acp{GP} que nous devront encoder et chacun d'entre eux sera une entrée dans le dictionnaire de patrons de régime. Nous avons dû codé chaque patron de régime \emph{à la main} dans Python puisque les cadres syntaxiques de VerbNet ne pouvaient pas être manipulés pour rendre des patrons de régime prêt à utiliser avec GenDR. Cependant, pour accélérer ce processus manuel, nous avons défini des objets qui représentent des actants syntaxiques non-étiquettés incorporant des traits lexicaux et syntaxiques. Ensuite, nous avons défini une fonction permettant d'assigner une étiquette au contenu d'un actant syntaxique, puis la combinaison du contenu et de l'étiquette nous a permi de créer des patrons de régime assez rapidement. Le tout sera expliqué en détails dans cette section.

\subsection{Contenu lexical et syntaxique d'un actant non-étiquetté}

Dans le premier bloc, nous définissons des objets comme \texttt{subj} ou \texttt{dir\_N} qui correspondent à des propriétés syntaxiques qui seraient normalement contenues dans un actant syntaxique. Par exemple, l'objet \texttt{subj} contient les traits suivants \lstinline|subj = 'rel=subjective dpos=N'|. Nous avons ainsi défini tous les propriétés lexico-syntaxiques possibles des actants que nous avons relevé parmi les identifiants de \acp{GP}. Autrement dit, ces objets correspondent à des raccourcis pour faciliter l'encodage des \acp{GP} individuels. Ensuite, nous avons créé une fonction pour prendre ces raccourcis et leur assigner une étiquette selon leur position dans le \ac{GP}. Cela nous évitera de décrire les propriétés syntaxiques des actants à chaque patron de régime. Bref, nous avons décrit tous les patrons de régime à l'aide de ces raccourcis: \lstinline|{'NP_agent_V_NP': [subj, dir_N],}|. Pour l'instant, ces objets n'incorporent que des propriétés syntaxiques, mais elles ne sont pas encore étiquettées (I,II, III, etc.).

\subsection{Assignation des étiquettes syntaxiques profondes aux actants}

L'objectif de la combinaison des fonctions \emph{roman} et \emph{gp} est d'assigner les étiquettes des actants syntaxiques selon leur position dans l'encodate d'un \ac{GP}. Par exemple, dans \lstinline|NP_agent_V_NP: [subj, dir_N]|, puisque \texttt{dir\_N} est à la deuxième position, les propriétés qu'il renferme seront encodés dans un actant syntaxique \texttt{II}. Le produit final d'une telle mécanique permet de générer le patron de régime: \lstinline|NP_agent_V_NP { I={rel=subjective dpos=N} II={rel=dir_objective dpos=N} }|. De plus, cette fonction nous permet aussi de dédoubler le même actant syntaxique pour un \ac{GP} dont l'un des actants syntaxiques se réalise à l'aide de deux prépositions différentes. Cela est illustré par le \ac{GP} \lstinline|NP_asset_V_NP_PP_from_out_of| qu'on a décrit à l'aide des objets: \lstinline|[subj, dir_N, [from_N, out_of_N]]|. Le résultat nous fournit \lstinline| III={rel=oblique dpos=N prep=from}| et \lstinline|III={rel=oblique dpos=N prep="out of"}| pour décrire le troisième actant syntaxique. 

\subsubsection{Création du gpcon}
Finalement, après avoir manuellement décrit tous les \acp{GP} répertoriés, nous leur imposons la fonction fonction \emph{gp} pour que l'étiquette de l'actant syntaxique soit réalisé et pour que le contenu des objets prédifinis soient générés à l'intérieur de l'étiquette. Un exemple du contenu final de ce dictionnaire est illustré à la figure~\ref{fig:4entries-gpcon}.

\begin{lstlisting}[language=Python, caption = Création du dictionnaire de patrons de régime]

# BLOCK 1 SYNTACTIC ACTANTS PROPERTIES
#subjective
subj = 'rel=subjective dpos=N'

#direct obj
dir_N = 'rel=dir_objective dpos=N'
dir_V_ING = 'rel=dir_objective dpos=V finiteness=GER'
dir_V_INF = 'rel=dir_objective dpos=V finiteness=INF'

#indirect obj
to_N = 'rel=indir_objective dpos=N prep=to'
indir_N = 'rel = indir_objective dpos = N'

#oblic
on_V = 'rel=oblique dpos=V prep=on'
to_obl_N = 'rel=oblique dpos=N prep=to' 
for_obl_N = 'rel=oblique dpos=N prep=for'
against_N = 'rel=oblique dpos=N prep=against'
...

# BLOCK 2 ASSIGN SYNTACTIC LABEL to ACTANT
def roman(n):
    return ['I', 'II', 'III', 'IV', 'V', 'VI'][n-1] # transforms arabic numbers in roman numbers
def gp(name, real_actant):
    s = name + ' {\n'
    i=0
    for actant in real_actant:
        i = i+1                   # starts to enumerate at 1
        if type(actant) == list:  # if not actant but list, apply function to actants in list
            for y in actant:
                s = s + "   " + roman(i) + "={" + y + "}\n"
        else:
            s = s + "   " + roman(i) + "={" + actant + "}\n" # apply function to actant
    s = s + '}\n'
    return s 

# BLOCK 3 CONSTRUCTION OF A GP ENTRY
descriptions = {
'NP_agent_V': [subj],
'NP_agent_V_NP': [subj, dir_N],
'NP_asset_V_NP_PP_from_out_of': [subj, dir_N, [from_N, out_of_N]],
...

# BLOCK 4 GPCON CREATION
with open('gpcon.dict','w') as f: 
    f.write('gpcon {\n')
    for d in descriptions.keys():       # for each gps descriptions,
        f.write(gp(d, descriptions[d])) # write them with the correct syntactic label
    f.write('}')
\end{lstlisting}

\section{Scripts pour l'évaluation de GenDR}

Pour évaluer la qualité de l'implémentation de VerbNet à GenDR, nous avons extrait les phrases exemples qui figurent dans VerbNet dans le but d'en faire des inputs et de tenter de les générer. Autrement dit, nous avons pris les phrases exemples accompagnant chaque cadre syntaxique, et nous voulions créer des graphes sémantiques à partir de ces phrases, dans le but de les donner à GenDR pour qu'il tente de générer la phrase de départ.

\subsection{Extraction des exemples}

L'\textbf{extraction des exemples} se fait grâce à la fonction \emph{treeframe} que nous reprenons du script ayant servi à extraire les identifiants de \acp{GP} (voir figure~\ref{fig:archivn}). L'objectif est de parcourir les fichiers XML pour uniquement récupérer les phrases exemples contenues dans les classes verbales et sous-classes.

Le second bloc de code s'occupe de la \textbf{compilation des phrases} extraites dans le fichier \emph{phrases.txt}, en les divisant ligne par ligne. 

\begin{lstlisting}[language=Python, caption = Extraction des phrases exemples de VerbNet]
# BLOCK 1 EXAMPLES EXTRACTION
def treeframes(t):
    z = []
    for frame in t.findall('FRAMES/FRAME'): # for each syntactic frame
        description = re.sub(r"\s*[\s\.\-\ +\\\/\(\)]\s*", '_',
				frame.find('DESCRIPTION').get('primary'))
        if description in exclude:
            continue    
        examples = [e.text for e in frame.findall('EXAMPLES/EXAMPLE')] # get the examples
        z =  z + examples 
    subclasses = t.findall('SUBCLASSES/VNSUBCLASS')
    subframes = [treeframes(subclass) for subclass in subclasses] #repeat operation for subclasses
    subframes = sum(subframes, []) # flatten list of lists
    return z + subframes

# BLOCK 2 LIST OF SENTENCES IN THE FILE phrases.txt
liste=[]
with open('phrases.txt','w') as f:
    for file in [f for f in os.listdir('verbnet') if f[-4:] == '.xml']:
        root = ET.parse('verbnet/'+file).getroot()       
        d = (treeframes(root))  # Applies treeframes function to all of VerbNet files
        finale_liste = liste + d
        [f.write(x+'\n') for x in finale_liste] # returns line after each example

\end{lstlisting}

\subsection{Création des structures sémantiques}\label{sec:pythonstruc}

Ce script crée les graphes de bases qui nous permettront de faire les tests. Ces inputs en préparation sont dépourvus de n\oe{}uds et d'arcs. Ils ne contiennent que des éléments non-sémantiques qui faciliteront l'encodage des inputs.

Pour ce faire, nous ouvrons le fichier \emph{phrases.txt} qui contient chaque phrase exemple, ligne par ligne. Le script créera 978 structures sémantiques vides pour les 978 phrases extraites de VerbNet. Il identifiera chaque structure de 001 à 978. Les stuctures créées par ce script ne contiendront que le texte à représenter sémantiquement et les crochets \{ \} nécessaires pour encadrer le graphe. Finalement, le script créera 978 structures.

\begin{lstlisting}[language=Python, caption = Création des structures sémantiques vides, label=structurepython]
phrases = open('phrases.txt','r')

with open('structures.str','w') as f: # create a .str structure
    for(i,p) in enumerate(phrases):   # for each sentence
        with open('s'+str(i)+'.str','w') as g:
            structure = 'structure Sem S'+str(i)+' # name each structure by enumeration
						{\n S {text="'+p.strip()+'"\n\n main-> \n }\n}' # insert as texte the sentence
            f.write(structure)
            g.write(structure)
\end{lstlisting}

