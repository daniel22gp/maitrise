\chapter{Python}

\draft{relire cette section pour que ce soit plus fluide et agréable à lire. Faire attention aux débuts de paragraphe et s'assurer qu'il y a une transition entre les sous-sections}

À l'aide du module \emph{xml.etree.cElementTree} créé pour le langage de programmation Python, nous avons pu faire des opérations sur l'ensemble des données de VerbNet qui sont encodées en XML. Le module \emph{xml.etree.cElementTree} prend les fichiers XML et les réorganise en \emph{element tree}. Cela nous permet de naviguer dans les fichiers XML avec des fonctions prévues pour ce module. C'est ainsi qu'on a pu manipuler et extraire les données de VerbNet qui nous intéressaient. Par après, nous les avons compilées dans des fichiers\emph{dict} qui seront utilisés par notre réalisateur de texte GenDR. Nous nous sommes aussi servis des fichiers XML et de Python pour extraire les phrases exemples accompagnant les cadres syntaxiques pour évaluer notre réalisateur de texte.

\section{Création du dictionnaire lexical}

La première étape de notre projet consistait à créer le dictionnaire verbal. Bien que le dictionnaire final comporte 6393 entrées lexicales verbales, cette partie de code nous permettait de créer la base de notre système. Il serait important de décrire dans les grandes lignes à quoi ressemble le dictionnaire pour mieux expliquer pourquoi nous l'avons extrait de cette manière. Ainsi MATE possède la caractéristique d'héritage des traits. Donc, en ce qui concerne les verbes, nous avons architecturé la chose de cette manière. Il existe une classe des verbes qui ont les traits suivants: une dpos=V et une spos=verb.

\begin{lstlisting}[language=XML]
verb {
  dpos = V
  spos = verb
}
\end{lstlisting}

Ensuite, les 278 classes verbales de VerbNet pointent vers la classe verb, donc les classes héritent de ces deux traits. Ce qui fait en sorte qu'on n'a pas a répété dans chaque entrée de classe verbale les traits dpos et spos. 

\begin{lstlisting}[language=XML]
"absorb-39.8": verb {
  gp = { id=NP_V_NP  
	       dia=12 } // Cotton absorbs water.
  gp = { id=NP_V_NP_PP_from_source  
	       dia=123 } // Cattle take in nutrients from their feed.
}
\end{lstlisting}

Ensuite, chaque membre de chaque classe verbale pointe vers la classe ou la sous-classe qui le représente ce qui fait en sorte qu'il hérite de tous les traits de la classe(sous-classe) vers laquelle il pointe. Par exemple: absorb, ingest, take in vont tous trois hériter des traits de l'entrée "absorb-39.8" qui est la classe verbale. C'est de cette manière que notre dictionnaire passe de 278 entrées lexicales à 6393. Car, les verbes de la langue anglaise ont été classés dans des classes verbales et nous les intégront à notre dictionnaire en gardant la même architecture que VerbNet avait pensé (basé sur les travaux de Levin).

\begin{lstlisting}[language=XML]
absorb: "absorb-39.8"
take_in: "absorb-39.8"
ingest_1: "absorb-39.8"
\end{lstlisting}

\subsection{Extractions de l'architecture hiérarchique et des 278 classes verbales de VerbNet}

\begin{lstlisting}[language=Python, caption = Script pour générer 'lexicon.dict']
# BLOC 1
def supers(t, i):
    ID = t.get('ID')
    sc = {ID:i}
    subclasses = t.findall('SUBCLASSES/VNSUBCLASS')
    if len(subclasses) > 0:
        for sub in subclasses:
            sc = {**sc, **supers(sub, ID)}
    return sc
		
# BLOC 2
def treeframes(t):
    ID = t.get('ID')
    z = []
    for frame in t.findall('FRAMES/FRAME'):
        description = re.sub(r"\s*[\s\.\-\ +\\\/\(\)]\s*", '_', 
        frame.find('DESCRIPTION').get('primary'))
        if description in exclude:
            continue
        description = re.sub('PP', 'PP_{}', description)
        preps = [p.get('value') or 
                p.find('SELRESTRS/SELRESTR').get('type').upper() 
                for p in frame.findall('SYNTAX/PREP')+frame.findall('SYNTAX/LEX')]
        preps = [sorted(p.split()) for p in preps]     
        examples = [e.text for e in frame.findall('EXAMPLES/EXAMPLE')]
        if len(preps)==1:
            description = description.format('_'.join(preps[0]))
        elif len(preps)==2:
            description = description.format('_'.join(preps[0]),
                                             '_'.join(preps[1]))
        elif len(preps)==3:
            description = description.format('_'.join(preps[0]), 
                                             '_'.join(preps[1]), 
                                             '_'.join(preps[2]))
        z.append((description, examples))
        
    subclasses = t.findall('SUBCLASSES/VNSUBCLASS')
    subframes = [treeframes(subclass) for subclass in subclasses]
    subframes = sum(subframes, []) # flatten list of lists
    return [(ID, z)] + subframes
		
# BLOC 3
with open('lexicon.dict','w') as f:
    f.write('lexicon {\n')
    for file in [f for f in os.listdir('verbnet') if f[-4:] == '.xml']:
        root = ET.parse('verbnet/'+file).getroot()       
        d = dict(treeframes(root))
        sc = supers(root, 'verb')
        for c in d.keys():
            f.write('"'+c+'"')
            if sc[c] == 'verb':
                f.write(': ' +sc[c] + ' {')
            else:
                f.write(': ' +'"'+sc[c]+'"' + ' {')
            [f.write('\n  gp = { id=' + gp[0] + (max(len(gp[0]), 30)-len(gp[0]))
                     *' ' + ' dia=x } // ' + ' '.join(gp[1])) for gp in d[c]]
            f.write('\n}\n')
    f.write('\n}')
\end{lstlisting}

\subsubsection{Premier Bloc}
-objectif de la fonction: la fonction \emph{supers} sert à recréer le mécanisme d'héritage qui compose l'architecture de VerbNet. Grâce à cette fonction, on peut faire en sorte que chaque classe fille hérite des traits de sa classe mère. 

-Explication brève de la fonction: Brièvement, cette fonction va chercher les identifiants de la classe mère, puis de toutes les sous-classes et sous-sous-classes. Puis elle crée un dictionnaire où la clé est la fille et la valeur est la mère: dictionnaire = {fille: mère}. Ce dictionnaire peut ensuite être utilisé par GenDR car comme nous l'avons vu à la section X, une lexie peut hériter des traits d'une entrée si on la fait pointer vers cette entrée. C'est exactement ce mécanisme d'héritage des traits que nous reprenons ici. Grâce à cela, nous n'avons pas à répéter toutes les informations comprises dans la classe mère à la classe fille, puisqu'effectivement elle hérite de tous les gps qui la compose. Mais grâce à cela, elle en hérite sans qu'on ait à les avoir dans l'entrée et ça fait en sorte que notre dictionnaire n'est pas saturé d'information. Puis nous utilisons l'aspect hiérarchique de VerbNet à notre avantage.

-Explication précise du code:
d'abord on défini la fonction supers qui prendra deux arguments. t et i. t est le fichier XML de la classe verbale et i est une variable. Pour chaque arbre, on va aller chercher son identifiant. puis on va créer un dictionnaire dont la clé sera l'identifiant et la valeur la variable i. Puis on commande à Python de naviguer dans l'arbre XML pour aller jusqu'à la balise SUBCLASSES qui contient toutes les sous-classes et les sous-sous-classes. Pour chaque sous-classes (pour les besoins de la cause sous-classe fait référence à la fois aux sous-classes et aux sous-sous-classes) nous mettons à jour le dictionnaire pour qu'il comprenne à la fois lui-même tel qu'il était au début de la fonction et on passe la fonction super aux sous-classe. Car les sous-classes sont architecturés de la même manière que les classes. On fait donc passer la fonction supers à ces sous-classes. donc supers prend sub comme premier argument, qui est dans le fond l'arbre XMl commençant à chaque sous-classe. Puis rend en sortie ID qui on se rappelle est une valeur fixe. Dans ce cas, ID a été cherché dans l'arbre initial et c'était le ID de la classe supérieure. Ce qui fait en bout de ligne qu'on se ramasse avec un dictionnaire comprenant ID de la mère: i , ID de la fille: ID de la mère. Ce morceau de dictionnaire sera réutilisé par le bloc 3.

\subsubsection{Deuxième Bloc}
objectif:la fonction treeframes est de nous donner pour chaque fichier XML (donc classe verbale) le ID et les descritions(cadre de sous-cat) et phrase exempled attachés à ce ID pour chaque classe, sous-classe et sous-sous-classe. spray-23-1 NP-V-NP Paul sprayed the wall. Cette information est celle qui peuplera notre lexique.

explication courte: On commence par chercher l'identifiant de la classe puis on va dans la balise des cadres pour aller chercher les descriptions (identifiant des patrons de régime). Ensuite on moule les descriptions pour qu'elle fonctionne dans le formalisme de MATE et à chaque fois qu'il y a des groupes prépositionnels PP dans la description on va chercher la préposition qui domine ce syntagme pour l'ajouter dans la description même \draft{je me rappelle pu pourquoi on avait fait ça, à voir} puis on va chercher les exemples associés à chaque cadre de sous-cat. On finit avec un identifiant qui pointe vers les descriptions des cadres de sous-cat et les exemples associés à chacun d'entre eux. Le tout est ensuite fait à toute les sous-classes.

description précise: on définit d'abord la fonction treeframes. Elle prend un arbre en argument,l'arbre est le fichier XML. on va chercher le ID, puis on initialise une liste vide qui sera utile plus tard. Ensuite pour chaque frame dans FRAMES on va chercher les descriptions à chaque fois qu'on tombe sur un frame en parcourant le doc XML. On va d'abord utiliser un nombre d'expression régulières pour modéliser les descriptions pour qu'elles fittent dans notre système MATE. nous retirons les espaces, les points, les tirets, les barres obliques, les paranthèses, tout cela et nous les remplaçons par des underscores. Si une description fait partie de la liste exclude. Alors on skip la description et on passe à la prochaine description. Ensuite on remplace chaque string PP par une string PP\_\{\}. On fait cela car les crochets seront remplacés par d'autres string plus tard grâce à la fonction format. Nous procédons ensuite à aller chercher toutes les prépositions à l'intérieur des frames syntaxiques. Ceux-ci sont encodés dans les PP. Puis pour chaque cadre syntaxique nous mettons les prépositions en ordre alphabétique dans une liste preps. Puis on extrait les exemples qui se retrouvent dans la balise exemple qui accompagne chaque description dans chaque frame. Puis finalement nous utilisons la fonction format qui nous permet de joindre chaque préposition dans la liste prep et de la subsituer aux crochets dans les descriptions pour que les descriptions aient maintenant les prépositions dans les descriptions directement. De cette manière on identifie mieux les descriptions des patrons de régime. Les descriptions et exemples sont ensuite mis dans une liste z que nous avions initilaisé au début. Puis nous répétons le même processus pour les sous-classes et sous-sous-classes. au final Identifiant et description,exemple. 

\subsubsection{Troisième Bloc}

objectif: avoir un dictionnaire qui contient les classes verbales de VerbNet avec tous les descriptions de leur patron de régime \draft{exemple de description} et les exemples qui corresponondent à ceux-ci. On veut aussi que le dictionnaire soit configuré de manière à tirer profit du mécanisme d'héritage de VerbNet que le logiciel MATE permet.

explication courte: On ouvre un fichier lexicon.dict dans lequel on va écrire le dictionnaire à l'aide des deux fonctions que nous avions créées aux blocs 1 et 2. Puis nous allons passer ces fonctions à l'ensemble des documents XML qui composent VerbNet. Ensuite, nous allons écrire à l'aide de la fonction write la manière que nous voulons que les ouputs de ces fonctions soient présentés. Nous allons fonc faire en sorte que chaque classe verbale non-dominée pointe vers la chaîne de caractère 'verb' pour qu'elles héritent de la classe 'verb' dans MATE. Cela fait en sorte que tous les classes verbales non-dominées hériterons de la dpos, spos,..\draft{vérifier les autres traits} de 'verb' sans qu'on ait à les réécrie à chaque fois. Puis toutes les classes dominées pointe vers les classes les dominant afin qu'elles héritent des traits de leur mère. Puis une fois qu'on a fait ça, on ajoute les descriptions et exemples associés à chaque classe verbale (et sous-classe). Pour que ça donne \draft{montrer un exemple avec lstinline}

description: on ouvre un fichier appelé lexicon.dict. Puis pour chaque fichier dans le repository qui contient tous les fichiers XML peuplant VerbNet, on va les parser avec le module xml etree  et on va aller chercher la racine de chaque fichier XML. À partir de la racine on peut faire les opérations qu'on veut: treeframes et supers. En appliquant treeframes à root, puis encode le oupout dans un dictionnaire appelé 'd'. Puis on applique supers à root. Puis pour chaque clé de d (les ID) on écrit la clé puis on la fait pointer vers la classe qui la domine, si il n'y a pas de classe qui la domine, alors on la fait pointer vers la chaîne 'verb'. Puis à ce point on a ID(class):ID(classe dominante ou 'verb'). Il nous manque les descriptions des patrons de régime et les exemples. On va écrire ceux-ci dans un format lisible par MATE, puis le résultat final ressemble à \draft {mettre un court exemple du résultat.}

\subsection{Extraction des 6393 membres des classes verbales pour enrichir le dictionnaire lexicon.dict}

Maintenant que nous avons extrait les classes verbales et l'architecture hiérarhique de VerbNet. Il ne nous reste qu'à peupler le dictionnaire des membres qui appartienent à chaque classe verbale. De cette manière notre lexicon.dict sera agrémenté de 6393 entrées lexicales désambiguisées. Effectivement à cette étape on procède aussi à la désambiguisation des verbes puisque plusieurs d'entre eux pointent vers des classes verbales différentes en fonction du sens dans lequel on les emploie. Cela a nécessairement un impact sur les patrons de régime qu'ils ont et nous voulons que notre dictionnaire tienne compte de cette diversité.

\begin{lstlisting}[language=Python, caption = Code pour ajouter des lexèmes à lexicon.dict]
#Bloc 1
def treemember(t):
    ID = t.get('ID')
    members = [m.get('name') for m in t.findall('MEMBERS/MEMBER')]
    subclasses = t.findall('SUBCLASSES/VNSUBCLASS')
    submembers = []
    if len(subclasses) > 0:
        for sub in subclasses:
            submembers = submembers + treemember(sub)
    return [(ID, members )] + submembers

# Bloc 2
files = [f for f in os.listdir('verbnet') if f[-4:] == '.xml']
members = dict(sum([treemember(ET.parse('verbnet/'+file).getroot())
 for file in files], [])) # ici on a classe: [membre,...]
values = sum(list(members.values()), []) # ici c'est uniquement les membres
 sans infos sur leur classe
dups = {m:[ID for ID in members.keys() if m in members[ID]]
 for m in values if values.count(m)>1}
unique_member = {m:ID for ID in members.keys()
 for m in values if m in members[ID] and values.count(m)==1}
lexemes = {d[0]+'_'+str(n+1):d[1][n]
 for d in dups.items() for n in range(len(d[1]))}
# Ici, je fusionne les dictionnaires ensemble
unified_dict = {**unique_member, **lexemes}

#Bloc 3
with open('members.dict','w') as f:
    f.write('members {\n')
    for key in sorted(unified_dict.keys()):
        f.write(key)
        f.write(': '),
        f.write('"'+str(unified_dict[key])+'"')
        f.write('\n')
    f.write('\n}\n')

\end{lstlisting}

\subsubsection{Bloc 1}
objectif: Pour chaque classe verbale, on va chercher les membres leur étant associés.

description courte: On défini la fonction treemember comme étant une fonction qui prend un arbre xml comme argument. On va analyser cet arbre pour chercher les informations qu'on veut. D'abord on veut le ID (la classe verbale comme telle) puis pour chaque balise MEMBER qu'on trouve en parcourant l'arbre XML on va regarder pour les membres dans cette balise. On va répéter la chose pour les sous-classes. Puis on exige que la fonction retourne en output une liste de tuples classe verbale et membres associés.

explication:

\subsubsection{Bloc 2}
objectif:
description courte:
explication:

\subsubsection{Bloc 3}
objectif:
description courte:
explication:

\section{Création du gpcon}
Faire un dictionnaire de patron de régime. On a jugé bon de créer un second dictionnaire qui ne contiendrait que les gps, autrement dit un gpcon. l'information sur les patrons de régime (les actants syntaxiques). Il existe x nombre de gps répertoriés. Il nous fallait les créer, car tel que mentionné, nous ne pouvions pas extraire les gps de VerbNet dû à une différence trop grande (cadre théorique et application). Notre système de GAT fonctionne avec la théorie Sens-Texte et nous pensons que c'est la théorie qui s'y prête le plus pour faire ce type d'opérations et qui tient le mieux compte de la manière dont le langage fonctionne.


\begin{lstlisting}[language=Python, caption = code pour gpcon.dict]
#Bloc 1
def roman(n):
    return ['I', 'II', 'III', 'IV', 'V', 'VI'][n-1]
def gp(name, real_actant):
    s = name + ' {\n'
    i=0
    for actant in real_actant:
        i = i+1
        if type(actant) == list:
            for y in actant:
                s = s + "   " + roman(i) + "={" + y + "}\n"
        else:
            s = s + "   " + roman(i) + "={" + actant + "}\n"
    s = s + '}\n'
    return s

#Bloc 2

#SUBJECTIVE
subj = 'rel=subjective dpos=N'

#DIRECT OBJECTIVE
dir_N = 'rel=dir_objective dpos=N'
dir_V_ING = 'rel=dir_objective dpos=V finiteness=GER'
dir_V_INF = 'rel=dir_objective dpos=V finiteness=INF'

#INDIRECT OBJECTIVE
to_N = 'rel=indir_objective dpos=N prep=to'
indir_N = 'rel = indir_objective dpos = N'

#OBLIQUE
on_V = 'rel=oblique dpos=V prep=on'
to_obl_N = 'rel=oblique dpos=N prep=to' 
for_obl_N = 'rel=oblique dpos=N prep=for'
as_N =  'rel=oblique dpos=N prep=as'
against_N = 'rel=oblique dpos=N prep=against'
at_N = 'rel=oblique dpos=N prep=at'
...

# LOC

locab = 'rel=oblique dpos=N prep=locab'
locad = 'rel=oblique dpos=N prep=locad'
locin = 'rel=oblique dpos=N prep=locin'

descriptions = {
'NP_agent_V': [subj],
'NP_agent_V_NP': [subj, dir_N],
'NP_asset_V_NP_PP_from_out_of': [subj, dir_N, [from_N, out_of_N]],
'NP_attribute_V': [subj],
'NP_attribute_V_NP_extent': [subj, dir_N],
'NP_attribute_V_PP_by_extent': [subj,by_N],
'NP_cause_V_NP': [subj, dir_N ],
'NP_instrument_V_NP': [subj, dir_N],
'NP_location_V': [subj],
'NP_location_V_NP_theme': [subj,dir_N],
'NP_location_V_PP_with_agent': [subj, with_N],
'NP_location_V_PP_with_theme': [subj, with_N],
'NP_material_V_NP': [subj,dir_N],
'NP_material_V_PP_into_product': [subj, into_N],
...

#Bloc 3
# CREATION DU GPCON
with open('gpcon.dict','w') as f:
    f.write('gpcon {\n')
    for d in descriptions.keys():
        f.write(gp(d, descriptions[d]))
    f.write('}')

\end{lstlisting}

\section{Scripts pour faire les tests}

Nous allons faire des tests pour vérifier si l'extraction que nous avons fait de VerbNet est bonne. prendre les phrases exmeples et créer des graphes sémantiques correspondant à chaque phrase exemplifiant un gp pour une classe verbale donnée. Le but est de passer ce graphe sémantique à GenDR 2.0 pour ensuite analyser les outputs qui en sortent. Si les outputs sont bel et bien les RSyntP correspondant aux phrases en entrée, alors on jugera que le système fonctionne bien. Cela nous permettra de voir ce qui fonctionne de ce qui ne fonctionne pas. Évidemment il y aurait des problèmes techniques et théoriques et c'est ce que nous tenterons de départir. Le chapitre suivant est dédié à l'implémentation et l'évaluation du système.

\subsection{Extraction des exemples}

On extrait les exemples des frames pour les mettre dans un fichier phrases.txt qui sera utilisé pour créer les structures (le bon nombre) et insérer le texte des phrases dans le graphe sémantique.

\begin{lstlisting}[language=Python, caption = code pour créer phrases.txt]
#Bloc 1
def treeframes(t):
    z = []
    for frame in t.findall('FRAMES/FRAME'):
        description = re.sub(r"\s*[\s\.\-\ +\\\/\(\)]\s*", '_',
				frame.find('DESCRIPTION').get('primary'))
        if description in exclude:
            continue    
        examples = [e.text for e in frame.findall('EXAMPLES/EXAMPLE')]
        z =  z + examples 
    subclasses = t.findall('SUBCLASSES/VNSUBCLASS')
    subframes = [treeframes(subclass) for subclass in subclasses]
    subframes = sum(subframes, []) # flatten list of lists
    return z + subframes

#Bloc 2
liste=[]
with open('phrases.txt','w') as f:
    for file in [f for f in os.listdir('verbnet') if f[-4:] == '.xml']:
        root = ET.parse('verbnet/'+file).getroot()       
        d = (treeframes(root))
        final_liste = liste + d
        [f.write(x+'\n') for x in final_liste]

\end{lstlisting}

\subsubsection{Bloc 1}
objectif:
explication courte:
description:

\subsubsection{Bloc 2}
objectif:
explication courte:
description:

\subsection{Création des structures sémantiques}

Cette section crée les graphes de bases qui nous permettront de faire les tests. Ces graphes sont dépourvus de noeuds et d'arcs, ils contiennent uniquement le cadre nécessaire pour qu'ensuite, on écrire manuellement les inputs tels que nous les avons vu à la section \draft{pointer vers la bonne section}. Effectivement, dans l'exemple que nous avions vu, il y avait toutes les informations requises pour que GenDR lise la structure d'input. Ici, crée le cadre dans lequel on va écrire les noeuds et les relations et on insère la phrase à créer pour faciliter l'écriture manuelle des graphes d'input.

\begin{lstlisting}[language=Python, caption = code pour créer des structures .str]
phrases = open('phrases.txt','r')

with open('structures.str','w') as f:
    for(i,p) in enumerate(phrases):
        with open('s'+str(i)+'.str','w') as g:
            structure = 'structure Sem S'+str(i)+'
						{\n S {text="'+p.strip()+'"\n\n main-> \n }\n}'
            f.write(structure)
            g.write(structure)
\end{lstlisting}

objectif:
explication courte:
description:
