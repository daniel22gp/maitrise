\chapter{Python}

À l'aide du module \emph{xml.etree.cElementTree} créé pour le langage de programmation Python, nous avons pu faire des opérations sur l'ensemble des données de VerbNet qui sont encodées en XML. Le module \emph{xml.etree.cElementTree} prend les fichiers XML et les réorganise en \emph{element tree}. Cela nous permet de naviguer dans les fichiers XML avec des fonctions prévues pour ce module. C'est ainsi qu'on a pu manipuler et extraire les données de VerbNet qui nous intéressaient. Par après, nous les avons compilées dans des fichiers\emph{dict} qui seront utilisés par notre réalisateur de texte GenDR. Nous nous sommes aussi servis des fichiers XML et de Python pour extraire les phrases exemples accompagnant les cadres syntaxiques pour évaluer notre réalisateur de texte.

\section{Création du dictionnaire lexical}

La première étape de notre projet consistait à créer le dictionnaire verbal. Bien que le dictionnaire final comporte 6393 entrées lexicales verbales, cette partie de code nous permettait de créer la base de notre système. Il serait important de décrire dans les grandes lignes à quoi ressemble le dictionnaire pour mieux expliquer pourquoi nous l'avons extrait de cette manière. Ainsi MATE possède la caractéristique d'héritage des traits. Donc, en ce qui concerne les verbes, nous avons architecturé la chose de cette manière. Il existe une classe des verbes qui ont les traits suivants: une dpos=V et une spos=verb.

\begin{lstlisting}[language=XML]
verb {
  dpos = V
  spos = verb
}
\end{lstlisting}

Ensuite, les 278 classes verbales de VerbNet pointent vers la classe verb, donc les classes héritent de ces deux traits. Ce qui fait en sorte qu'on n'a pas a répété dans chaque entrée de classe verbale les traits dpos et spos. 

\begin{lstlisting}[language=XML]
"absorb-39.8": verb {
  gp = { id=NP_V_NP  
	       dia=12 } // Cotton absorbs water.
  gp = { id=NP_V_NP_PP_from_source  
	       dia=123 } // Cattle take in nutrients from their feed.
}
\end{lstlisting}

Ensuite, chaque membre de chaque classe verbale pointe vers la classe ou la sous-classe qui le représente ce qui fait en sorte qu'il hérite de tous les traits de la classe(sous-classe) vers laquelle il pointe. Par exemple: absorb, ingest, take in vont tous trois hériter des traits de l'entrée "absorb-39.8" qui est la classe verbale. C'est de cette manière que notre dictionnaire passe de 278 entrées lexicales à 6393. Car, les verbes de la langue anglaise ont été classés dans des classes verbales et nous les intégront à notre dictionnaire en gardant la même architecture que VerbNet avait pensé (basé sur les travaux de Levin).

\begin{lstlisting}[language=XML]
absorb: "absorb-39.8"
take_in: "absorb-39.8"
ingest_1: "absorb-39.8"
\end{lstlisting}

\subsection{Création du dictionnaire initial contenant les 278 classes verbales comme entrées lexicales}

\begin{lstlisting}[language=Python, caption = Script pour générer `lexicon.dict']
# BLOC 1
def supers(t, i):
    ID = t.get('ID')
    sc = {ID:i}
    subclasses = t.findall('SUBCLASSES/VNSUBCLASS')
    if len(subclasses) > 0:
        for sub in subclasses:
            sc = {**sc, **supers(sub, ID)}
    return sc
		
# BLOC 2
def treeframes(t):
    ID = t.get('ID')
    z = []
    for frame in t.findall('FRAMES/FRAME'):
        description = re.sub(r"\s*[\s\.\-\ +\\\/\(\)]\s*", '_', 
        frame.find('DESCRIPTION').get('primary'))
        if description in exclude:
            continue
        description = re.sub('PP', 'PP_{}', description)
        preps = [p.get('value') or 
                p.find('SELRESTRS/SELRESTR').get('type').upper() 
                for p in frame.findall('SYNTAX/PREP')+frame.findall('SYNTAX/LEX')]
        preps = [sorted(p.split()) for p in preps]     
        examples = [e.text for e in frame.findall('EXAMPLES/EXAMPLE')]
        if len(preps)==1:
            description = description.format('_'.join(preps[0]))
        elif len(preps)==2:
            description = description.format('_'.join(preps[0]),
                                             '_'.join(preps[1]))
        elif len(preps)==3:
            description = description.format('_'.join(preps[0]), 
                                             '_'.join(preps[1]), 
                                             '_'.join(preps[2]))
        z.append((description, examples))
        
    subclasses = t.findall('SUBCLASSES/VNSUBCLASS')
    subframes = [treeframes(subclass) for subclass in subclasses]
    subframes = sum(subframes, []) # flatten list of lists
    return [(ID, z)] + subframes
		
# BLOC 3
with open('lexicon.dict','w') as f:
    f.write('lexicon {\n')
    for file in [f for f in os.listdir('verbnet') if f[-4:] == '.xml']:
        root = ET.parse('verbnet/'+file).getroot()       
        d = dict(treeframes(root))
        sc = supers(root, 'verb')
        for c in d.keys():
            f.write('"'+c+'"')
            if sc[c] == 'verb':
                f.write(': ' +sc[c] + ' {')
            else:
                f.write(': ' +'"'+sc[c]+'"' + ' {')
            [f.write('\n  gp = { id=' + gp[0] + (max(len(gp[0]), 30)-len(gp[0]))
                     *' ' + ' dia=x } // ' + ' '.join(gp[1])) for gp in d[c]]
            f.write('\n}\n')
    f.write('\n}')
\end{lstlisting}

\subsubsection{Premier Bloc}
-objectif de la fonction: la fonction \emph{supers} sert à recréer le mécanisme d'héritage qui compose l'architecture de VerbNet. Grâce à cette fonction, on peut faire en sorte que chaque classe fille hérite des traits de sa classe mère. 

-Explication brève de la fonction: Brièvement, cette fonction va chercher les identifiants de la classe mère, puis de toutes les sous-classes et sous-sous-classes. Puis elle crée un dictionnaire où la clé est la fille et la valeur est la mère: dictionnaire = {fille: mère}. Ce dictionnaire peut ensuite être utilisé par GenDR car comme nous l'avons vu à la section X, une lexie peut hériter des traits d'une entrée si on la fait pointer vers cette entrée. C'est exactement ce mécanisme d'héritage des traits que nous reprenons ici. Grâce à cela, nous n'avons pas à répéter toutes les informations comprises dans la classe mère à la classe fille, puisqu'effectivement elle hérite de tous les gps qui la compose. Mais grâce à cela, elle en hérite sans qu'on ait à les avoir dans l'entrée et ça fait en sorte que notre dictionnaire n'est pas saturé d'information. Puis nous utilisons l'aspect hiérarchique de VerbNet à notre avantage.

-Explication précise du code:
d'abord on défini la fonction supers qui prendra deux arguments. t et i. t est le fichier XML de la classe verbale et i est une variable. Pour chaque arbre, on va aller chercher son identifiant. puis on va créer un dictionnaire dont la clé sera l'identifiant et la valeur la variable i. Puis on commande à Python de naviguer dans l'arbre XML pour aller jusqu'à la balise SUBCLASSES qui contient toutes les sous-classes et les sous-sous-classes. Pour chaque sous-classes (pour les besoins de la cause sous-classe fait référence à la fois aux sous-classes et aux sous-sous-classes) nous mettons à jour le dictionnaire pour qu'il comprenne à la fois lui-même tel qu'il était au début de la fonction et on passe la fonction super aux sous-classe. Car les sous-classes sont architecturés de la même manière que les classes. On fait donc passer la fonction supers à ces sous-classes. donc supers prend sub comme premier argument, qui est dans le fond l'arbre XMl commençant à chaque sous-classe. Puis rend en sortie ID qui on se rappelle est une valeur fixe. Dans ce cas, ID a été cherché dans l'arbre initial et c'était le ID de la classe supérieure. Ce qui fait en bout de ligne qu'on se ramasse avec un dictionnaire comprenant ID de la mère: i , ID de la fille: ID de la mère. Ce morceau de dictionnaire sera réutilisé par le bloc 3.

\subsubsection{Deuxième Bloc}
objectif:la fonction treeframes est de nous donner pour chaque fichier XML (donc classe verbale) le ID et les descritions(cadre de sous-cat) et phrase exempled attachés à ce ID pour chaque classe, sous-classe et sous-sous-classe. spray-23-1 NP-V-NP Paul sprayed the wall. Cette information est celle qui peuplera notre lexique.

explication courte: On commence par chercher l'identifiant de la classe puis on va dans la balise des cadres pour aller chercher les descriptions (identifiant des patrons de régime). Ensuite on moule les descriptions pour qu'elle fonctionne dans le formalisme de MATE et à chaque fois qu'il y a des groupes prépositionnels PP dans la description on va chercher la préposition qui domine ce syntagme pour l'ajouter dans la description même \draft{je me rappelle pu pourquoi on avait fait ça, à voir} puis on va chercher les exemples associés à chaque cadre de sous-cat. On finit avec un identifiant qui pointe vers les descriptions des cadres de sous-cat et les exemples associés à chacun d'entre eux. Le tout est ensuite fait à toute les sous-classes.

description précise: on définit d'abord la fonction treeframes. Elle prend un arbre en argument,l'arbre est le fichier XML. on va chercher le ID, puis on initialise une liste vide qui sera utile plus tard. Ensuite pour chaque frame dans FRAMES on va chercher les descriptions à chaque fois qu'on tombe sur un frame en parcourant le doc XML. On va d'abord utiliser un nombre d'expression régulières pour modéliser les descriptions pour qu'elles fittent dans notre système MATE. nous retirons les espaces, les points, les tirets, les barres obliques, les paranthèses, tout cela et nous les remplaçons par des underscores. Si une description fait partie de la liste exclude. Alors on skip la description et on passe à la prochaine description. Ensuite on remplace chaque string PP par une string PP\_\{\}. On fait cela car les crochets seront remplacés par d'autres string plus tard grâce à la fonction format. Nous procédons ensuite à aller chercher toutes les prépositions à l'intérieur des frames syntaxiques. Ceux-ci sont encodés dans les PP. Puis pour chaque cadre syntaxique nous mettons les prépositions en ordre alphabétique dans une liste preps. Puis on extrait les exemples qui se retrouvent dans la balise exemple qui accompagne chaque description dans chaque frame. Puis finalement nous utilisons la fonction format qui nous permet de joindre chaque préposition dans la liste prep et de la subsituer aux crochets dans les descriptions pour que les descriptions aient maintenant les prépositions dans les descriptions directement. De cette manière on identifie mieux les descriptions des patrons de régime. Les descriptions et exemples sont ensuite mis dans une liste z que nous avions initilaisé au début. Puis nous répétons le même processus pour les sous-classes et sous-sous-classes. au final Identifiant et description,exemple. 

\subsubsection{Troisième Bloc}

objectif: créer le fichier .dict qui sera la moitié de notre lexicon final. L'autre moitié est la prochaine section \draft{référence}. Donc le but est de créer un dictionnaire en utilisant les fonctions créées dans les bloc 1 et 2 et les fichiers XML qui composent VerbNet. Ce dictionnaire sera composé d'unité pointant vers la classe qui les englobe et où les patrons de régime et exemple sont là et le mécanisme d'héritage est employé. Puis l'objectif est de créer le dictionnaire pour qu'il soit utilisable par MATE, donc on le crée d'une certaine manière.
explication courte:
description:

%Pour extraire les descriptions des patrons de régime nous avons utilisé deux fonctions. D'abord la fonction \emph{treeframe} qui s'occupe de récupérer la description de chaque frame syntaxique à travers tout VN. Nous passons à travers tous les frames existant dans les XML de VN. Autant dans les classes que les sous-classes. Toutefois, cette fonction ne fait pas que récupérer la description du syntactic frame de VN et nous la recrache tel quel. Nous faisons quelques opérations sur les descriptions que nous extrayons. Notamment, nous remplaçons tout espace,tiret, point,barre oblique, paranthèses qui pourraient se trouver dans les descriptions par des {\_} à l'aide d'expressions régulières. Puis, nous retirons certaines descriptions de gp à cause de leur caractèrere problématique à encoder(pour des raisons théoriques, logiques -- à expliquer ailleurs).Puis une fois qu'un clean up a été fait des descriptions et que nous avons uniquement celles que nous voulions, nous procédons à une seconde étape de raffinement des descriptions. Nous utiliserons une seconde fois une expression régulière pour trouver tous les occurrences de PP car nous ajouterons les prépositions impliquées dans les PP comme tel. Pour ce faire, nous faisons un search de tous les prépositions existant dans les patrons de régime de verbnet(et non dans la description, c'est là que les patrons de régime de VN nous ont été utiles) et nous mettons les prépositions que nous soutirons des gp directement dans le nom de la description des gp à la suite du mot PP à chaque fois qu'on retrouve le mot PP dans une description. On obtient ainsi les descriptions nettoyées de celle qu'on ne veut pas, avec uniquement des underscore pour séparer les syntagmes puis les prépositions (lorsqu'il y a lieu) dans les noms des descriptions de gp.

\subsection{Extraction des 6393 membres des classes verbales pour enrichir le dictionnaire lexicon.dict}

\begin{lstlisting}[language=Python, caption = code pour ajouter des lexèmes à lexicon.dict]
def treemember(t):
    ID = t.get('ID')
    members = [m.get('name') for m in t.findall('MEMBERS/MEMBER')]
    subclasses = t.findall('SUBCLASSES/VNSUBCLASS')
    submembers = []
    if len(subclasses) > 0:
        for sub in subclasses:
            submembers = submembers + treemember(sub)
    return [(ID, members )] + submembers

files = [f for f in os.listdir('verbnet') if f[-4:] == '.xml']

members = dict(sum([treemember(ET.parse('verbnet/'+file).getroot()) for file in files], [])) # ici on a classe: [membre,...]
values = sum(list(members.values()), []) # ici c'est uniquement les membres, sans infos sur leur classe
dups = {m:[ID for ID in members.keys() if m in members[ID]] for m in values if values.count(m)>1}
unique_member = {m:ID for ID in members.keys() for m in values if m in members[ID] and values.count(m)==1}
lexemes = {d[0]+'_'+str(n+1):d[1][n] for d in dups.items() for n in range(len(d[1]))}

# Ici, je fusionne les dictionnaires ensemble
unified_dict = {**unique_member, **lexemes}

with open('members.dict','w') as f:
    f.write('members {\n')
    for key in sorted(unified_dict.keys()):
        f.write(key)
        f.write(': '),
        f.write('"'+str(unified_dict[key])+'"')
        f.write('\n')
    f.write('\n}\n')

\end{lstlisting}

\section{Création du gpcon}

\begin{lstlisting}[language=Python, caption = code pour gpcon.dict]
def roman(n):
    return ['I', 'II', 'III', 'IV', 'V', 'VI'][n-1]
		
def gp(name, real_actant):
    s = name + ' {\n'
    i=0
    for actant in real_actant:
        i = i+1
        if type(actant) == list:
            for y in actant:
                s = s + "   " + roman(i) + "={" + y + "}\n"
        else:
            s = s + "   " + roman(i) + "={" + actant + "}\n"
    s = s + '}\n'
    return s


#SUBJECTIVE
subj = 'rel=subjective dpos=N'

#DIRECT OBJECTIVE
dir_N = 'rel=dir_objective dpos=N'
dir_V_ING = 'rel=dir_objective dpos=V finiteness=GER'
dir_V_INF = 'rel=dir_objective dpos=V finiteness=INF'

#INDIRECT OBJECTIVE
to_N = 'rel=indir_objective dpos=N prep=to'
indir_N = 'rel = indir_objective dpos = N'

#OBLIQUE
on_V = 'rel=oblique dpos=V prep=on'
to_obl_N = 'rel=oblique dpos=N prep=to' 
for_obl_N = 'rel=oblique dpos=N prep=for'
as_N =  'rel=oblique dpos=N prep=as'
against_N = 'rel=oblique dpos=N prep=against'
at_N = 'rel=oblique dpos=N prep=at'
...

# LOC

locab = 'rel=oblique dpos=N prep=locab'
locad = 'rel=oblique dpos=N prep=locad'
locin = 'rel=oblique dpos=N prep=locin'

descriptions = {
'NP_agent_V': [subj],
'NP_agent_V_NP': [subj, dir_N],
'NP_asset_V_NP_PP_from_out_of': [subj, dir_N, [from_N, out_of_N]],
'NP_attribute_V': [subj],
'NP_attribute_V_NP_extent': [subj, dir_N],
'NP_attribute_V_PP_by_extent': [subj,by_N],
'NP_cause_V_NP': [subj, dir_N ],
'NP_instrument_V_NP': [subj, dir_N],
'NP_location_V': [subj],
'NP_location_V_NP_theme': [subj,dir_N],
'NP_location_V_PP_with_agent': [subj, with_N],
'NP_location_V_PP_with_theme': [subj, with_N],
'NP_material_V_NP': [subj,dir_N],
'NP_material_V_PP_into_product': [subj, into_N],
...

# CREATION DU GPCON
with open('gpcon.dict','w') as f:
    f.write('gpcon {\n')
    for d in descriptions.keys():
        f.write(gp(d, descriptions[d]))
    f.write('}')

\end{lstlisting}

Il serait important d'expliquer pourquoi nous n'avons pas repris les rôles sémantiques fournis par VerbNet, car la fonction gp et l'entièreté de ce script repose sur ces fondements de TST. Melcuk explique d'abord pourquoi nous étiquettons nos actants sémantiques de cette manière, puis pourquoi les rôles sémantiques n'ont pas leur place en sémantique.



Soit le mentionner ici, ou ailleurs, mais il a fallu faire un dictionnaire de patron de régime. D'abord, parce qu'on s'est rendu compte que du à toute l'information qu'on allait chercher et la différence dans le type d'information, on a jugé bon de créer un second dictionnaire qui ne contiendrait que les gps, autrement dit un gpcon. Celui l'information sur les patrons de régime (les actants syntaxiques). Il existe x nombre de gps répertoriés. Nous les avons trouvé en faisant un ensemble à partir de tous les descriptions que nous avons obtenus avec le script précédent. Une fois que nous avons l'ensemble des gps différents. Il nous fallait les créer, car tel que mentionné, nous ne pouvions pas extraire les gps de VerbNet dû à une différence trop grande (cadre théorique et application). Notre système de GAT fonctionne avec la théorie Sens-Texte et nous pensons que c'est la théorie qui s'y prête le plus pour faire ce type d'opérations et qui tient le mieux compte de la manière dont le langage fonctionne. Ainsi, nous avons créer le gpcon à partir de Python car un bon nombre d'opérations peuvent être automatisés (éviter les fautes, et c'est plus transparent). Pour la création du gpcon, notre dictionnaire en Python ressemblait à ça. Nos keys() étaient la description du gp et les valeurs étaient les actants syntaxiques impliqués dans ce gp (avec de l'information sur les actants syntaxiques nécessitant une préposition à réaliser). Selon l'ordre dans lequel figure nos objets dans la liste qui est ce qu'on retrouve dans values(), notre fonction va assigner le bon actant syntaxique(I, II, III, etc.) ainsi, cette partie est automatisée grâce à cette fonction. Après, pour l'objet "subj" on va lui assigner une string 'rel=subjective dpos=N' ce qui est encodé dans une autre cellule. Ainsi à chaque fois qu'un gp a  un subj, on n'a pas à écrire ce que subj contient. Alors pour l'objet subj, on aura I et 'rel=subjective dpos=N'. C'est l'union de la fonction gp et de la fonction roman qui nous permettent d'assigner les bons actants syntaxiques aux objets dans la liste qui représente les values dans mon dictionnaire de gpcon.

\section{Scripts pour faire les tests}

\subsection{Extraction des exemples}

\begin{lstlisting}[language=Python, caption = code pour créer phrases.txt]
def treeframes(t):
    z = []
    for frame in t.findall('FRAMES/FRAME'):
        description = re.sub(r"\s*[\s\.\-\ +\\\/\(\)]\s*", '_', frame.find('DESCRIPTION').get('primary'))
        if description in exclude:
            continue    
        examples = [e.text for e in frame.findall('EXAMPLES/EXAMPLE')]
        z =  z + examples 
    subclasses = t.findall('SUBCLASSES/VNSUBCLASS')
    subframes = [treeframes(subclass) for subclass in subclasses]
    subframes = sum(subframes, []) # flatten list of lists
    return z + subframes

liste=[]
with open('phrases.txt','w') as f:
    for file in [f for f in os.listdir('verbnet') if f[-4:] == '.xml']:
        root = ET.parse('verbnet/'+file).getroot()       
        d = (treeframes(root))
        final_liste = liste + d
        [f.write(x+'\n') for x in final_liste]

\end{lstlisting}

\subsection{Création des structures qui serviront de tests}

Dans la figure ci-bas, on explique comment on a créer les documents .str qui serviront d'input à notre système MATE qui prend ce genre de document en entrée.

\begin{lstlisting}[language=Python, caption = code pour créer des structures .str]
phrases = open('phrases.txt','r')

with open('structures.str','w') as f:
    for(i,p) in enumerate(phrases):
        with open('s'+str(i)+'.str','w') as g:
            structure = 'structure Sem S'+str(i)+'{\n S {text="'+p.strip()+'"\n\n main-> \n }\n}'
            f.write(structure)
            g.write(structure)
\end{lstlisting}
