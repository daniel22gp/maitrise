%!TEX root = ../memoire.tex
\chapter{Évaluation}\label{ch:eval}

Avant d'entrer dans les détails de l'évaluation de notre système, il serait important de faire un bref retour sur les méthodes d'évaluation en \ac{GAT}. \cite{ReiterInvestigationValidityMetrics2009} expliquent qu'il existe trois types classiques d'évaluation. Ils nomment d'abord la méthode d'évaluation basée sur l'exécution d'une tâche en utilisant des rapports génénés automatiquement. Ils nomment aussi la méthode d'évaluation humaine, puis la méthode automatique. D'ailleurs, les auteurs remarquaient que les méthodes d'évaluation automatiques se faisaient de plus en plus populaires. Notamment, la méthode BLEU \draft{expliquer rapidement ça consiste à quoi} qui avait été développée, à la base, pour les systèmes de traductions automatiques. Nous ferons donc un bref survol de ces méthodes pour décider laquelle nous conviendra le plus.

BLEU a été créé, à la base, pour évaluer les rendements des traductions automatiques en comparant les outputs produits à un ensemble de traductions humaines qui servent de point de référence. Comme la traduction automatique et la \ac{GAT} comportent un aspect automatique, des chercheurs comme \cite{Habash2003MatadorAL, Langkilde-gearyForestbasedstatisticalsentence2000} ont estimé que la \ac{GAT} bénéficierait de cette méthode d'évaluation. Toutefois, lors de son passage à SemEval2017, le réalisateur profond FORGe \citep{DBLP:conf/semeval/MilleCBW17} a notamment été évalué par BLEU et les auteurs du projet en ont fait une brève critique. Ils soulignent que BLEU évalue effectivement bien la couverture, mais comporte des lacunes pour analyser la qualité des outputs d'un système. FORGe a reçu un score au dessus de la moyenne pour l'évaluation humaine, mais un score plus faible avec BLEU. Ils expliquent ce décalage en mentionnant que leur réalisateur mettait de l'avant la qualité de ses outputs par rapport à la quantité.

La méthode d'évaluation basée sur l'exécution d'une tâche est assez courante, selon \cite{ReiterInvestigationValidityMetrics2009}. Ces auteurs estiment qu'il s'agit de la méthode qui évalue le mieux le contenu des réalisations d'un système de \ac{GAT} puisqu'on se sert de celles-ci pour effectuer une tâche donnée. La fluiditié à réussir les étapes menant à l'accomplissment de la tâche découle principalement de la qualité des phrases générées. Toutefois, les auteurs nous préviennent que c'est malheureusement la méthode la plus coûteuse en termes de temps et de ressources. Cette méthode n'est pas toujours facile à mettre en place car il faut trouver des participants prêt à donner de leur temps pour lire les rapports générés et accomplir une tâche quelconque.

Finalement, il y a l'évaluation humaine qui est plus simple à faire que la méthode à base de tâche, mais plus lente que la méthode automatique. Toutefois, cela reste une méthode très populaire dans le domaine. Il s'agit de coter les outputs en fonction de leur performance à produire des phrases syntaxiquement et sémantiquement selon le jugement d'un évaluateur.

Considérant ces trois méthodes, nous devons en exclure la méthode basée sur l'exécution d'une tâche et les méthodes automatiques. D'abord, notre système ne réalise pas du texte dans un but précis, donc nous n'avons pas de tâches à effectuer pour tester la validité des réalisations. De plus, nous ne génèrons pas du texte linéaire, donc il aurait fallu trouver des participants capable de déchiffrer des arbres de dépendances. De plus, pour cette même raison, nous ne pouvons pas utiliser la méthode automatique car notre système génère des arbres cat cette méhode comparent des textes linéaires produit par des humains avec les générations du système. Il nous est donc impossible de comparer nos arbres syntaxiques de surface avec du texte. En fin de compte, une seule méthode d'évaluation s'offre à nous et c'est l'évaluation humaine.

\section{Mise en place de l'évaluation}
Pour procéder à l'évaluation de notre système, nous avons utilisé les outputs du script que représente la figure \ref{structurepython} (voir le chapitre \ref{ch:python}). Ceux-ci étaient des structures sémantiques vides (dépourvues de prédicats et d'arguments). Il n'y avait que le code pour encadrer le graphe et le texte à représenter sémantiquement. Nous avons comblé les 978 structures vides en y encodant les unités et relations sémantiques représentant le contenu de chaque énoncé. Ensuite, nous donnerons ces structures sémantiques en input à notre système et nous évaluerons les réalisations produites.

Comme nous avions une limite de temps, nous avons décidé de prendre un échantillon des 978 strucutres sémantiques, ce qui s'est traduit par la sélection aléatoire de 75 strucures. Parmi celles-ci, 25 ont servi pour une phase de \emph{développement} précédant la phase d'évaluation pour vérifier quels étaient les problèmes immédiats que nous pouvions régler afin que la partie \emph{évaluation} soit la plus arbitraire possible. La phase de développement nous a permi de constater que de nombreux inputs comportaient des erreurs d'encodage. Nous avons donc noté le type de problème que les inputs comportaient afin de corriger le tir pour la partie évaluation. La partie de développement nous a aussi permi de constater que certains lexèmes appartenant à des parties du discours différentes apparaissaient en double dans notre dictionnaire. Autrement dit, le verbe \lex{work} et le nom \lex{work} ont la même forme et le système ne sait pas comment les différencier. Cela a donc une incidence sur la réalisation d'une phrase incluant l'une ou l'autre de ces formes puisque le système construit l'arbre syntaxique à partir du premier lexème qu'il récupère. C'est pourquoi nous avons procédé à un filtrage massif de notre dictionnaire et nous avons réglé ces problèmes en créant des entrées sémantiques dans le \emph{semanticon} qui contiendra les deux entrées lexicales: une version verbale et une version nominale: \lstinline! work { lex = work_n  lex= work_2}!. Bref, lors de la phase de développement, nous n'avons pas évaluer la qualité des outputs encore.

Ensuite, nous avons passé au peigne fin chacun des inputs qui sera évalué, puis nous avons corrigé les problèmes d'homographie. Finalement, nous avons pu procéder à la phase de tests grâce aux 50 structures sémantiques restantes. Pour ce faire, nous avons développé un script qui a généré automatiquement toutes les représentations visuelles et textuelles des arbres de dépendances profonds et superficiels. La partie visuelle nous permettait de regarder les différentes constructions d'arbres rapidement pour identifier lesquelles étaient des réussites. La partie textuelle nous permettait de lire quelles étaient informations lexicales sur les n\oe{}uds dans les différentes phases d'arborisation car certaines informations ne sont pas explicitées dans le format graphique de présentation des outputs (patron de régime sélectionné, partie du discours demandée, diathèse effectuée, etc.)
                              
\section{Rappel}

Nous avons évalué le rappel ainsi:\[\frac{\text{nombre de structures attendues générées}}{\text{nombre de structures attendues}}\]. Cette méthode d'évaluation produit un rappel de 62\%. Ce score se situe au-dessous de nos attentes. Mais il est expliqué par plusieurs facteurs cruciaux dont la majorité peut être corrigée en peu de temps. \draft{entre le 20 et le 30 je vais refaire les calculs avec les corrections pour voir qu'est-ce que ça aurait donné dans le cas échéant.}

D'abord, il y a les erreurs d'encodage qui ont su échapper aux mailles du filet lors de notre filtrage post-développement. Il y avait très peu d'erreur dans l'input sémantique. Nous en avons relevé un seule. Il s'agit de l'emploi d'une mauvaise forme d'un verbe. Nous avons pris grill\_1 et il aurait fallu prendre grill\_3. L'utilisation d'une mauvaise acception d'un verbe mène à l'impossibilité de récupérer le bon patron de régime. En termes d'erreurs humaines, certaines se sont glissées dans les dictionnaires à notre insu. Notamment, une diathèse manquante pour un patron de régime donné. Cela mène à l'impossibilité de réaliser le gp. Ou alors, l'oubli d'une préposition dans une actant syntaxique. Cela fait en sorte qu'en surface, la phrase n'est pas celle qu'on attendait.

Les classes par défaut ont aussi leur lot de problèmes. Effectivement, nous avons dû créer une classe \texttt{quote} pour les patrons de régime qui sélectionne des paroles (Ex: Helen told Ellen "leave the room"). Toutefois, une erreur s'est probablement glissée dans l'encodage de cette classe et le système n'a pas pu la réaliser lors de l'arborisation. Nous avons aussi eu un problème avec les classes qui sélectionnent certaines prépositions. Le système n'est pas capable de récupérer cette préposition dans la classe par défaut, ce qui empêché de réaliser la phrase souhaitée. 

Nous avons eu beacoup de problèmes de rappel lié aux manque de données de la part de VerbNet. Effectivement, il y eut quelques cas de verbes utilisés dans les phrases exemples de VerbNet qui ne figuraient pas dans leur propre dictionnaire verbal. Par le fait même, les patrons de régime associés à ces verbes ne sont pas encodés. Cela mène à l'impossibilité de générer l'arbre qu'on se serait attendu de voir. beg, believe. C'était souvent le cas lorsque la phrase exemple contenait deux verbes. Comme on n'a pas les informations requises pour process le second verbe (diathèse et gp) l'arbre que nous pensions généré ne sera jamais retrieve par GenDR. 

De plus, parmi les problèmes qui nous sont hérités de VerbNet, il y aussi eu quelques cas où la structure sémantique de l'input demandait un patron de régime Y de la part du prédicat, mais que ce patron de régime ne figurait pas parmi la liste de gps de la classe associée à ce verbe. Ou bien he backed out of going  (go\_2: n'avait pas le gp pour réalsier on the trip)

Il y aussi des problèmes due à des incompatibilités sémantique-syntaxe entre VerbNet et la TST. Effectivement, nous n'avons pas tenté de créer une struture sémantique pour qu'elle plaise au patron de régime donné par VerbNet. Cela fait en sorte que quelques des tests allaient nécessairement échouer. Par exemple, la phrase \form{Tom jumped the horse over the fence} est régie par le verbe jump\_2 qui est exprimé par le gp suivant: \lstinline!gp = { id=NP_V_NP_PP_SPATIAL_location dia=123 }!.Les trois actants sémantiques étant \sem{Tom} \sem{horse} et \sem{fence} selon VerbNet. Toutefois, nous considérons que dans ce scénario, le verbe \lex{jump} en surface exprimerait plutôt \sem{faire sauter}. Il existe une fonction lexicale permettant ce genre de situation, mais nous ne l'avons pas encodé dans cette version de VerbNet puisque nous ne touchons pas aux fonctions lexicales. Nous avons noté d'autre cas d'incompatiblités théoriques lors de la phase de développement. 

Nous avons relevé beaucoup de cas d'utilisation de verbe support en surface ('X took a flight' ). Le gp utilisé pour take a flight est bon en soi, mais la représentation sémantique de take est flight ne devrait pas faire appel au gp de take, mais plutôt au gp de flight qui encode les fonctions lexicales (verbe support). et on ne perdrait pas le sens. le \sem{fly} incorpore les lexicalisations \lex{fly} et  \lex{flight}. Took est réalisé comme verbe support en récupérant la fonction lexicale encodée sous \lex{flight}. Nous avons aussi relevé d'autres types de collocations dont X  slept the sleep of the dead. Dans ce cas \form{the sleep of the dead} est une expression figée qui signifie une intensité. Elle modifie le verbe sleep. VerbNet laisse sous-entendre que sleep sélectionne un objet direct qui lui sélectionne un complément du nom. Mais en fait, il ne s'agit que d'un verbe modifié par un intensificateur.

Finalement, le dernier problème qui a le plus affecté le rappel est la défectuosité du mécanisme d'héritage des gps entre les classes dominées et les classes dominantes. Il s'agit du mécanisme dont nous vantions les mérites au début du chapitre. Concrètement, il s'agit d'un moyen qui fait en sorte qu'une class x pointe vers une classe y et hérite des traits encodés dans la classe y. Mais il se trouve que le mécanisme d'héritage des traits fonctionne partiellement. 

Tel que présenté dans le début du chapitre, les unités lexicales pointent vers des classes verbales, une classe verbale peut pointer vers une autre classe verbale, et les classe verbales non-dominées pointent vers la classe par défaut \texttt{verb}. Celle-ci donne les traits dpos=V et spos=verb à tous les lexèmes pointant vers des classes verbales. L'héritage de ce trait est réussi à travers les classes dominées et dominantes car le lexème de surface contient les traits. C'est pourquoi nous disons que le mécanisme fonctionne partiellement. Mais les patrons de régime ne semblent pas se transmettre d'une classe dominante à une classe dominée. Il semblerait que les traits \texttt{id} et \texttt{dia} qui sont encodés dans le trait \texttt{gp} ne sont pas transmis. Ce problème semble provenir de MATE qui n'est pas capable de transmettre l'héritage d'un trait à l'intérieur d'un autre trait. Pour mieux illustrer le concept, nous prendrons le cas de \form{Steved tossed the ball from the corner to the garden.}. Le sémantème \sem{toss\_3} pointait vers la classe \texttt{throw-17.1-1} qui est une classe dominée par "throw-17.1" qui est domineé par "throw-17.1". Cependant le patron de régime dont nous avions besoin pour réaliser la phrase souhaitée se trouvait dans le régime de la classe mère "throw-17.1". GenDR n'a donc pas été capable d'aller chercher les traits id et dia. Mais il a été capable de réaliser la racine de l'arbre avec le lexème \lex{throw\_3} en héritant du trait dpos de la classe par défaut \texttt{verb}.

Pour pallier à ce mécanisme d'héritage deffectueux, nous pourrions directement implémanter tous les gps des classes dominantes dans les classes dominées. Ainsi on garde l'architecture pensée de VerbNet où les classes héritent des patrons des autres classes. Le défaut de cette solution est que notre dictionnaire sera saturé d'information puisqu'il existe énormément de sous-classe. Toutefois, cette solution est facilement adaptable puisque nous avons encore les scripts Python ayant servi à extraire les patrons de régime et à créer le lexicon. Nous n'avons qu'à modifier le script pour que chaque sous-classe hérite explicitement des gps des classe qui les domine. Évidemment, cela aurait aussi un impact direct sur la précision, puisqu'on a plus de chance de générer une erreur si plusieurs gps sont disponibles.

\section{Précision}

Nous avons noté la précision de cette manière:\[\frac{\text{nombre de structures correctes}}{\text{nombre de structures générées}}\]. Cela nous donne une précision de 77\%.

Les erreurs humaines que nous avions mentionnées pour le rappel ont généralement un impact sur la précision. Par exemple, en mettant la mauvaise désambiguisation d'un verbe dans l'input, alors les patrons de régime utilisés pour réaliser la phrase ne seront pas les bons. Cela peut générer de bonnes phrases, si par chance un bon gp s'y trouve. Mais on a vu que cela pouvait générer des phrases incongrues: \form{She grilled the steak on me} ou \form{She grilled the steak in me}.

Les erreurs de VerbNet affectent aussi la précision de l'output négativement. Lorsqu'un verbe utilisé dans l'exemple n'est pas répertorié par VerbNet. Le système utilise ses règles de secours de lexicalisation et va tout de même tenter de réaliser quelque chose. C'est le cas d'une phrase ayant le verbe \lex{do} pour \form{I begged her to do it}. Comme GenDR n'a pas hérité de l'entrée que VerbNet aurait dû avoir, le système a tenté diverses réalisations. Certaines d'entre elles échoueront, mais d'autres seront réalisées en surface. C'est le cas de \form{I begged her for the do.}. Dans ce cas, GenDR a supposé que c'était possiblement un nom. Et comme il existait un patron de régime de \lex{beg} permettant cette construction, alors le système a réalisée cette incongruité. Nous avons relevé d'autres cas similaires dans notre évaluation.

Les informations extraites de VerbNet nous ont aussi fait réaliser des phrases quasi-correctes mais bizarres. C'est une conséquence des patrons de régime qui permettent 2 à 4 prépositions différentes pour la réalisation d'une actant syntaxique en surface. Le système va donc générer deux arbres différents corrects. Chacun d'entre eux aura une préposition différente pour l'actant concerné, mais généralement seule l'une d'entre elle génère une phrase grammaticale. Ainsi, GenDR a généré les phrases \form{The doctor cured pat of pneumonia} et \form{The doctor cured pat out of pneumonia}. La deuxième est un arbre bien construit, mais dont la sémantique est fautive.

Un autre type d'erreur de précision provient de la manière dont GenDR génère les arbres. Tel que nous l'avons vu plus tôt, le système crée x nombres de racines pour un input où x est le nombre de patron de régime inclu pour une classe verbale. 

Ensuite, on sélectionne les gps et on vérifie que la diathèse permet l'appplication du gp sélectionné. C'est là que le problème prend forme. GenDR peut ainsi sélectionner un gp fautif qui respecte la diathèse et les contraintes sur le n\oe{}ud. Mais, il est possible que l'arborisation échoue parce que l'un des actants syntaxiques choisi contient une préposition qui rendra la phrase agrammaticale. Concrètement pour régler ce problème, il faudrait revoir l'application de nos règles. Par exemple la phrase \form{The street gushed}. Rien dans notre système nous empêchait de réaliser cette forme fautive.

\section{Synthèse de l'évaluation}

\draft{Est-ce que c'est pertinent que je parle de la f1 ? Elle est de 69\%}
\draft{Est-ce que j'ai besoin de faire un retour sur ce qui a été dit ? Ou bien j'en parle dans ma conclusion finale}

Ce qu'on retire de tout cela sont deux problèmes majeurs. Le mécanisme d'héritage qui fonctionne partiellement avec l'architecture présente qui nuit au rappel.

De plus, il faut aussi tenir compte du fait que nous avons testé sur 5\% des structures d'input. Mais nous pensons que c'est un 5\% significatif. Les mêmes erreurs revenaient souvent dans l'analyse des résultats.

