%!TEX root = ../memoire.tex
\chapter{Évaluation}\label{ch:eval}

Avant d'entrer dans les détails de l'évaluation de notre système, il serait important de faire un bref retour sur les méthodes d'évaluation en \ac{GAT}. \cite{ReiterInvestigationValidityMetrics2009} expliquent qu'il existe trois types classiques d'évaluation. Ils nomment d'abord la méthode d'évaluation basée sur l'exécution d'une tâche en utilisant des rapports génénés automatiquement. Ils nomment aussi la méthode d'évaluation humaine, puis la méthode automatique. D'ailleurs, les auteurs remarquaient que les méthodes d'évaluation automatiques se faisaient de plus en plus populaires. Notamment, la méthode BLEU \draft{expliquer rapidement ça consiste à quoi} qui avait été développée, à la base, pour les systèmes de traductions automatiques. Nous ferons donc un bref survol de ces méthodes pour décider laquelle nous conviendra le plus.

BLEU a été créé, à la base, pour évaluer les rendements des traductions automatiques en comparant les outputs produits à un ensemble de traductions humaines qui servent de point de référence. Comme la traduction automatique et la \ac{GAT} comportent un aspect automatique, des chercheurs comme \cite{Habash2003MatadorAL, Langkilde-gearyForestbasedstatisticalsentence2000} ont estimé que la \ac{GAT} bénéficierait de cette méthode d'évaluation. Toutefois, lors de son passage à SemEval2017, le réalisateur profond FORGe \citep{DBLP:conf/semeval/MilleCBW17} a notamment été évalué par BLEU et les auteurs du projet en ont fait une brève critique. Ils soulignent que BLEU évalue effectivement bien la couverture, mais comporte des lacunes pour analyser la qualité des outputs d'un système. FORGe a reçu un score au dessus de la moyenne pour l'évaluation humaine, mais un score plus faible avec BLEU. Ils expliquent ce décalage en mentionnant que leur réalisateur mettait de l'avant la qualité de ses outputs par rapport à la quantité.

La méthode d'évaluation basée sur l'exécution d'une tâche est assez courante, selon \cite{ReiterInvestigationValidityMetrics2009}. Ces auteurs estiment qu'il s'agit de la méthode qui évalue le mieux le contenu des réalisations d'un système de \ac{GAT} puisqu'on se sert de celles-ci pour effectuer une tâche donnée. La fluiditié à réussir les étapes menant à l'accomplissment de la tâche découle principalement de la qualité des phrases générées. Toutefois, les auteurs nous préviennent que c'est malheureusement la méthode la plus coûteuse en termes de temps et de ressources. Cette méthode n'est pas toujours facile à mettre en place car il faut trouver des participants prêt à donner de leur temps pour lire les rapports générés et accomplir une tâche quelconque.

Finalement, il y a l'évaluation humaine qui est plus simple à faire que la méthode à base de tâche, mais plus lente que la méthode automatique. Toutefois, cela reste une méthode très populaire dans le domaine. Il s'agit de coter les outputs en fonction de leur performance à produire des phrases syntaxiquement et sémantiquement selon le jugement d'un évaluateur.

Considérant ces trois méthodes, nous devons en exclure la méthode basée sur l'exécution d'une tâche et les méthodes automatiques. D'abord, notre système ne réalise pas du texte dans un but précis, donc nous n'avons pas de tâches à effectuer pour tester la validité des réalisations. De plus, nous ne génèrons pas du texte linéaire, donc il aurait fallu trouver des participants capable de déchiffrer des arbres de dépendances. De plus, pour cette même raison, nous ne pouvons pas utiliser la méthode automatique car notre système génère des arbres cat cette méhode comparent des textes linéaires produit par des humains avec les générations du système. Il nous est donc impossible de comparer nos arbres syntaxiques de surface avec du texte. En fin de compte, une seule méthode d'évaluation s'offre à nous et c'est l'évaluation humaine.

\section{Mise en place de l'évaluation}
Pour procéder à l'évaluation de notre système, nous avons utilisé les outputs du script que représente la figure \ref{structurepython} (voir le chapitre \ref{ch:python}). Ceux-ci étaient des structures sémantiques vides (dépourvues de prédicats et d'arguments). Il n'y avait que le code pour encadrer le graphe et le texte à représenter sémantiquement. Nous avons comblé les 978 structures vides en y encodant les unités et relations sémantiques représentant le contenu de chaque énoncé. Ensuite, nous donnerons ces structures sémantiques en input à notre système et nous évaluerons les réalisations produites.

Comme nous avions une limite de temps, nous avons décidé de prendre un échantillon des 978 strucutres sémantiques, ce qui s'est traduit par la sélection aléatoire de 75 strucures. Parmi celles-ci, 25 ont servi pour une phase de \emph{développement} précédant la phase d'évaluation pour vérifier quels étaient les problèmes immédiats que nous pouvions régler afin que la partie \emph{évaluation} soit la plus arbitraire possible. La phase de développement nous a permi de constater que de nombreux inputs comportaient des erreurs d'encodage. Nous avons donc noté le type de problème que les inputs comportaient afin de corriger le tir pour la partie évaluation. La partie de développement nous a aussi permi de constater que certains lexèmes appartenant à des parties du discours différentes apparaissaient en double dans notre dictionnaire. Autrement dit, le verbe \lex{work} et le nom \lex{work} ont la même forme et le système ne sait pas comment les différencier. Cela a donc une incidence sur la réalisation d'une phrase incluant l'une ou l'autre de ces formes puisque le système construit l'arbre syntaxique à partir du premier lexème qu'il récupère. C'est pourquoi nous avons procédé à un filtrage massif de notre dictionnaire et nous avons réglé ces problèmes en créant des entrées sémantiques dans le \emph{semanticon} qui contiendra les deux entrées lexicales: une version verbale et une version nominale: \lstinline! work { lex = work_n  lex= work_2}!. Bref, lors de la phase de développement, nous n'avons pas évaluer la qualité des outputs encore.

Ensuite, nous avons passé au peigne fin chacun des inputs qui sera évalué, puis nous avons corrigé les problèmes d'homographie. Finalement, nous avons pu procéder à la phase de tests grâce aux 50 structures sémantiques restantes. Pour ce faire, nous avons développé un script qui a généré automatiquement toutes les représentations visuelles et textuelles des arbres de dépendances profonds et superficiels. La partie visuelle nous permettait de regarder les différentes constructions d'arbres rapidement pour identifier lesquelles étaient des réussites. La partie textuelle nous permettait de lire quelles étaient informations lexicales sur les n\oe{}uds dans les différentes phases d'arborisation car certaines informations ne sont pas explicitées dans le format graphique de présentation des outputs (patron de régime sélectionné, partie du discours demandée, diathèse effectuée, etc.)
                              
\section{Rappel}

Nous avons mesuré le rappel de cette façon: 
\[\frac{\text{nombre de structures attendues générées}}{\text{nombre de structures attendues}}\].

Cette évaluation nous donne un taux de 62\%, ce qui se situe au-dessous de nos attentes, mais ce score est expliqué par plusieurs facteurs cruciaux dont la majorité peut être corrigée en peu de temps. \draft{entre le 20 et le 30 je vais refaire les calculs avec les corrections pour voir qu'est-ce que ça aurait donné dans le cas échéant.}

D'abord, ce qui a nuit au rappel sont les erreurs d'encodage qui ont su échapper aux mailles du filet, lors de notre filtrage post-développement. Il y avait très peu d'erreurs dans les inputs, dont l'emploi de la mauvaise acception d'un vocable. Par exemple, nous avons inséré grill\_1 dans la structure qui pointe vers la classe \texttt{cooking-45.3}, mais nous aurions du insérer grill\_3 qui pointe vers la bonne classe selon l'usage de ce verbe dans l'exemple. L'utilisation d'une mauvaise acception d'un verbe mènera inévitablement à l'impossibilité de récupérer le bon patron de régime. Ensuite, dans les dictionnaires, d'autres erreus se sont glissés comme une diathèse manquante pour un patron de régime donné ou alors, l'oubli d'une préposition pour la réalisation de surface d'actant syntaxique.

Les classes par défaut ont aussi leur lot de problèmes. Effectivement, nous avons dû créer une classe \texttt{quote} pour les patrons de régime qui sélectionne des paroles (Ex: \form{Helen told Ellen "leave the room"}). Toutefois, une erreur s'est probablement glissée dans l'encodage de cette classe et le système n'a pas pu la réaliser lors de l'arborisation. Nous avons aussi eu un problème avec les classes (comme nom-propre, endroit,) qui n'ont pas pu réaliser en surface les prépositions sélectionnées, bien que nous l'avions encodé dans le système.

Ensuite, nous avons eu beacoup de problèmes de rappel lié aux manques de données de la part de VerbNet. Effectivement, il y eut quelques cas de verbes utilisés dans les phrases exemples de VerbNet qui ne figuraient pas dans leur propre dictionnaire verbal. Par le fait même, les patrons de régime associés à ces verbes ne sont pas encodés, ce qui mène à l'impossibilité de générer l'arbre attendu. C'était le cas de la phrase \form{He begged her to do it} où le verbe \lex{DO} manquait à l'appel. Ce type de situation était récurrente lorsque la phrase exemple détenait deux verbes, comme on n'a pas les informations requises (\texttt{dia} et \texttt{id}) pour traiter le second verbe. Toutefois, dans certains cas, ce n'était pas que le verbe manquait à l'appel, mais plutôt un de ses patrons de régime. Effectivement, nous avons eu la phrase \form{He backed out of going on the trip}, mais aucun des acceptions de \lex{go} ne détenait le régime nécessaire pour réaliser cette phrase.

Certains problèmes de rappel provenaient des incompatibilités sémantico-syntaxiques entre VerbNet et la \ac{TST}. Effectivement, nous avons représenté chaque phrase exemple en graphe sémantique de type \emph{Sens-Texte}. Comme les patrons de régime font le pont entre la sémantique et la syntaxe, s'il y a une incompatibilité entre la sémantique de l'énoncé et sa représentation syntaxique selon VerbNet, ça mènera à un échec. Par exemple, la phrase \form{The plays and ballets alternate} se traduit par un seul actant syntaxique selon VerbNet. Toutefois, nous considérons qu'il y a deux actants sémantiques, donc deux actants syntaxiques puisqu'ils sont réalisés en surface. Comme l'input sémantique que nous avons fourni au système contenait deux actants sémantiques, le \ac{GP} qui régit la construction (\texttt{NP\_V}) ne peut pas faire le pont entre notre interprétation sémantique et les constructions fournies par VerbNet. 

Nous avons, par ailleurs, relevé d'autres cas similaires d'incompatibilité sémantique comme la phrase \form{Gloria slept the sleep of the dead}. Dans cet exemple, \form{the sleep of the dead} est une expression figée qui représente un degré d'intensité. Sémantiquement parlant, cette expression modifie le verbe \lex{sleep}. Toutefois, VerbNet considère que \lex{sleep}(verbal) sélectionne l'objet direct \lex{sleep} (\emph{of the dead}). Ainsi, si nous avions eu à représenter cette construction syntaxique précisément, nous n'aurions jamais utilisé le patron de régime \texttt{NP\_V\_NP} (verbe transitif direct) puisque dans notre représentation il n'y aurait eu qu'un seul actant sémantique. Alors que dans \texttt{NP\_V\_NP}, VerbNet considère qu'il y a deux actants sémantiques. Ç'aurait été la fonction lexicale \lexfn{Magn}, que nous avons vu au chapitre \ref{chapgendr}, qui se serait chargée de réaliser la forme \form{to sleep the sleep of the dead}.

\subsection{Mécanisme d'héritage defectueux}
Le dernier problème qui a le plus affecté le rappel est la défectuosité du mécanisme d'héritage des \acp{GP} entre les classes dominées et les classes dominantes. Il s'agit du mécanisme dont nous vantions les mérites dans le chapitre précédent \ref{ch:implementation}. Concrètement, c'est ce qui permettait en théorie qu'une sous-classe \texttt{throw-17.1-1}hérite des \acp{GP} encodés dans la classe qui la domine \texttt{throw-17.1} sans qu'on ait à les expliciter dans la sous-classe même. Ce mécanisme d'héritage est utilisé à de nombreuses reprises dans GenDR, d'ailleurs dans l'ancienne version de ce système, il permettait au entrées lexicales verbales d'hériter de toutes les propriétés syntaxiques des classes verbales par défaut (intransitif, transitif direct/indirect, ditransitif). Cependant, il semblerait que le mécanisme ne fonctionne pas parfaitement.

Tel que présenté au chapitre précédent, dans la nouvelle version de GenDR, les unités lexicales pointent vers des classes verbales (unité : classe), puis une classe verbale peut pointer vers une autre classe verbale. Mais les classes verbales non-dominées pointent vers la classe par défaut \texttt{verb} pour en hériter les traits \texttt{dpos=V} et \texttt{spos=verb}. Selon notre évaluation, l'héritage de ce trait se rend jusqu'en syntaxe de surface, ce qui prouve que le mécanisme fonctionne partiellement puisque l'unité lexicalisée contient le trait alors qu'il a été hérité de la classe par défaut jusqu'à la sous-classe. 

Le transfert s'est donc correctement appliqué, mais les patrons de régime ne se sont pas transmis entre classes dominantes et classes dominées. Il semblerait que les traits \texttt{id} et \texttt{dia} qui sont encodés dans le trait \texttt{gp} n'ont pas pu se rendre puisque ceux-ci sont encodés à un niveau plus profond que le trait \texttt{dpos=V} par exemple. Autrement dit, le système peut permettre l'héritage d'un trait de premier niveau, mais est incapable de transmetter un trait enchâssé dans un trait. Malheureusement, nous avions construit notre architecture de cette manière, mais nous pourrions assez facilement réparer ce problème. 

Pour pallier à ce mécanisme d'héritage defectueux, nous pourrions directement implémenter, avec nos scripts Python, chaque \ac{GP} de chaque classe mère à chaque classe fille. De cette manière, on garde l'architecture pensée de VerbNet où les classes héritent des patrons des autres classes et règle le problème. Cependant, le défaut de cette solution est que notre dictionnaire sera beaucoup plus lourd puisqu'il existe énormément de sous-classe.

Pour visualiser l'héritage dont nous parlons, voici une courte mise en situation qui nous a été révélée lors de l'évaluation. La phrase \form{Steved tossed the ball from the corner to the garden.} n'a pas pu être rappelée puisque nous n'avions pas le régime nécessaire dans l'entrée du verbe. Le lexème{toss\_3} pointait vers la classe \texttt{throw-17.1-1} qui est une classe dominée \texttt{throw-17.1} qui est elle-même dominée par \texttt{throw-17.1}. Cependant le \ac{GP} dont nous avions besoin pour réaliser la phrase souhaitée se trouvait dans le régime de la classe mère \texttt{throw-17.1}. GenDR n'a donc pas été capable de récupérer les traits nécessaires à l'arborisation, mais il a été capable de réaliser la racine de l'arbre grâce à l'héritage du trait \texttt{dpos=V} encodé dans la classe par défaut \texttt{verb}.

\section{Précision}

Nous avons noté la précision de cette manière:\[\frac{\text{nombre de structures correctes}}{\text{nombre de structures générées}}\]. Cela nous donne une précision de 77\%.

Les erreurs humaines que nous avions mentionnées pour le rappel ont généralement un impact sur la précision. Par exemple, en mettant la mauvaise désambiguisation d'un verbe dans l'input, alors les patrons de régime utilisés pour réaliser la phrase ne seront pas les bons. Cela peut générer de bonnes phrases, si par chance un bon gp s'y trouve. Mais on a vu que cela pouvait générer des phrases incongrues: \form{She grilled the steak on me} ou \form{She grilled the steak in me}.

Les erreurs de VerbNet affectent aussi la précision de l'output négativement. Lorsqu'un verbe utilisé dans l'exemple n'est pas répertorié par VerbNet. Le système utilise ses règles de secours de lexicalisation et va tout de même tenter de réaliser quelque chose. C'est le cas d'une phrase ayant le verbe \lex{do} pour \form{I begged her to do it}. Comme GenDR n'a pas hérité de l'entrée que VerbNet aurait dû avoir, le système a tenté diverses réalisations. Certaines d'entre elles échoueront, mais d'autres seront réalisées en surface. C'est le cas de \form{I begged her for the do.}. Dans ce cas, GenDR a supposé que c'était possiblement un nom. Et comme il existait un patron de régime de \lex{beg} permettant cette construction, alors le système a réalisée cette incongruité. Nous avons relevé d'autres cas similaires dans notre évaluation.

Les informations extraites de VerbNet nous ont aussi fait réaliser des phrases quasi-correctes mais bizarres. C'est une conséquence des patrons de régime qui permettent 2 à 4 prépositions différentes pour la réalisation d'une actant syntaxique en surface. Le système va donc générer deux arbres différents corrects. Chacun d'entre eux aura une préposition différente pour l'actant concerné, mais généralement seule l'une d'entre elle génère une phrase grammaticale. Ainsi, GenDR a généré les phrases \form{The doctor cured pat of pneumonia} et \form{The doctor cured pat out of pneumonia}. La deuxième est un arbre bien construit, mais dont la sémantique est fautive.

Un autre type d'erreur de précision provient de la manière dont GenDR génère les arbres. Tel que nous l'avons vu plus tôt, le système crée x nombres de racines pour un input où x est le nombre de patron de régime inclu pour une classe verbale. 

Ensuite, on sélectionne les gps et on vérifie que la diathèse permet l'appplication du gp sélectionné. C'est là que le problème prend forme. GenDR peut ainsi sélectionner un gp fautif qui respecte la diathèse et les contraintes sur le n\oe{}ud. Mais, il est possible que l'arborisation échoue parce que l'un des actants syntaxiques choisi contient une préposition qui rendra la phrase agrammaticale. Concrètement pour régler ce problème, il faudrait revoir l'application de nos règles. Par exemple la phrase \form{The street gushed}. Rien dans notre système nous empêchait de réaliser cette forme fautive.

\section{Synthèse de l'évaluation}

\draft{Est-ce que c'est pertinent que je parle de la f1 ?}
\draft{Est-ce que j'ai besoin de faire un retour sur ce qui a été dit ? Ou bien j'en parle dans ma conclusion finale}

Ce qu'on retire de tout cela sont deux problèmes majeurs. Le mécanisme d'héritage qui fonctionne partiellement avec l'architecture présente qui nuit au rappel.

De plus, il faut aussi tenir compte du fait que nous avons testé sur 5\% des structures d'input. Mais nous pensons que c'est un 5\% significatif. Les mêmes erreurs revenaient souvent dans l'analyse des résultats.

