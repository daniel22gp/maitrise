%!TEX root = ../memoire.tex

\chapter{Évaluation}\label{ch:eval}

\cite{ReiterInvestigationValidityMetrics2009} expliquent qu'il existe trois types classiques d'évaluation en \ac{GAT}: l'évaluation basée sur l'exécution d'une tâche en utilisant des rapports génénés automatiquement, l'évaluation humaine, puis l'évaluation automatique. Nous ferons un bref survol de ces trois types avant de présenter notre propre évaluation.

L'évaluation humaine consiste à coter les outputs en fonction de leur performance à produire des phrases syntaxiquement et sémantiquement correctes selon le jugement d'évaluateurs. Cette méthode est plus simple à mettre en place que la méthode à base de tâche, mais plus lente que les automatiques. Toutefois, elle reste une méthode très populaire dans le domaine.\FL{tu pourrais élaborer un peu}

La méthode d'évaluation basée sur l'exécution de tâches est assez courante.\FL{faut expliquer en quoi ça consiste} \cite{ReiterInvestigationValidityMetrics2009} estiment que c'est celle qui évalue le mieux le contenu des réalisations d'un système de \ac{GAT} puisqu'on se sert de celles-ci pour effectuer une tâche donnée. \draft{La fluiditié à réussir les étapes menant à l'accomplissment de la tâche découle principalement de la qualité des phrases générées.}\FL{je comprends pas ce que tu veux dire} Toutefois, c'est aussi la méthode la plus coûteuse en termes de temps et de ressources puisqu'il faut trouver des participants prêt à donner de leur temps pour lire les rapports générés et accomplir une tâche donnée.

Parmi les méthodes automatiques, celle qui revient le plus souvent est le score BLEU, qui a été créé pour évaluer les systèmes de traduction automatique en comparant les traductions produites automatiquement à un ensemble de traductions humaines.\FL{comment se calcule BLEU?} Comme la traduction automatique et la \ac{GAT} comportent un aspect automatique, des chercheurs comme \cite{Langkilde-gearyForestbasedstatisticalsentence2000} et \cite{Habash2003MatadorAL} ont estimé que cette métrique s'appliquerait aussi à la \ac{GAT}. Toutefois, cette méthode d'évaluation est critiquée. Par exemple, \cite{DBLP:conf/semeval/MilleCBW17} soulignent que BLEU évalue effectivement bien la couverture, mais comporte des lacunes pour analyser la qualité des outputs d'un système. Dans cet article, ils argumentent que leur réalisateur, FORGe, obtient un score au-dessus de la moyenne pour l'évaluation humaine, mais un score plus faible avec BLEU, et expliquent ce décalage par le fait que leur réalisateur favorise la précision par rapport au rappel.

Pour notre recherche, les méthodes basées sur l'exécution de tâches et automatiques s'appliquent mal. D'abord, notre système ne réalise pas du texte devant servir un but précis, donc nous n'avons pas de tâches à effectuer pour tester la validité des réalisations. De plus, nous ne génèrons pas du texte fini, donc il faudrait trouver des participants capables de lire des arbres de dépendances. Pour cette même raison, nous ne pouvons pas utiliser la méthode automatique puisque notre système génère des arbres et que cette méhode compare des textes produits par des humains avec les générations du système. En fin de compte, la seule méthode d'évaluation qui s'offre à nous est l'évaluation humaine.

\section{Méthodologie d'évaluation}

Pour procéder à l'évaluation de notre système, nous avons utilisé les outputs du script que représente la figure \ref{structurepython} (voir le chapitre \ref{ch:python}). \draft{Ceux-ci étaient des structures sémantiques vides (dépourvues de prédicats et d'arguments). Il n'y avait que le code pour encadrer le graphe et le texte à représenter sémantiquement.}\FL{c'est pas pertinent, il suffit de dire qu'on a préparé les RSém des phrases, mais il faut dire d'où tu as tiré ces phrases!} Nous avons comblé les 978 structures vides en y encodant les unités et relations sémantiques représentant le contenu de chaque énoncé. Ensuite, nous donnerons ces structures sémantiques en input à notre système et nous évaluerons les réalisations produites.

Comme nous étions limités dans le temps, nous avons décidé de prendre un échantillon aléatoire de 75 des 978 strucutres sémantiques. Parmi celles-ci, 25 ont servi pour une phase de \emph{développement} précédant la phase d'évaluation pour vérifier quels étaient les problèmes immédiats que nous pouvions régler afin que la partie \emph{évaluation} soit la plus objective possible. La phase de développement nous a permi de constater que de nombreux inputs comportaient des erreurs d'encodage. Nous avons donc noté le type de problèmes que les inputs comportaient afin de corriger le tir pour la partie évaluation. La partie de développement nous a aussi permi de constater que certains lexèmes appartenant à des parties du discours différentes apparaissaient en double dans notre dictionnaire. Autrement dit, le verbe \lex{work} et le nom \lex{work} ont la même forme et le système ne sait pas comment les différencier, ce qui a une incidence sur la réalisation. Nous avons procédé à un filtrage massif de notre dictionnaire et nous avons réglé ces problèmes en créant des entrées sémantiques dans le \emph{semanticon} qui contiennent les deux entrées lexicales désambiguïsées: \lstinline!work {lex=work_n  lex=work_2}!. Ensuite, nous avons passé au peigne fin chacun des inputs à évaluer, puis nous avons corrigé les problèmes d'homographie.

Finalement, nous avons pu procéder à l'évaluation comme telle, sur les 50 structures sémantiques restantes. Pour ce faire, nous avons développé un script qui a généré automatiquement les arbres de dépendances profonds et superficiels à partir de ces 50 structures sémantiques. Les résultats que nous rapportons ici sont ceux de cet échantillon d'évaluation. Nous n'avons testé qu'une fois cet échantillon, donc nos résultats correspondent à la borne inférieure de performance de notre système, sans ajustements pour les inputs donnés.
                              
\section{Rappel}

Rappelons que pour chaque structure sémantique, le système peut produire plusieurs structures syntaxiques. Nous avons mesuré le rappel selon la formule suivante:\FL{comment tu définis "structure attendue"?}

\draft{je considère comme une structure attendue tous les gps applicables avec l'input qu'on a. Par exemple, si dans l'input je mets : Paul talked to Sara. Je m'attendrais à Paul talked with sara, Paul talked to Sara et Paul talked, puisque leur gp le permet. Ensuite, je regarde combien d'entre elles ont été générées.}

\[\text{Rappel} = \frac{\text{nombre de structures attendues générées}}{\text{nombre de structures attendues}}\]

Cette évaluation nous donne un rappel de 62\,\%,\FL{donne les chiffres bruts aussi, avec le nombre moyen de structures générées par input} ce qui se situe au-dessous de nos attentes, mais ce score s'explique par plusieurs facteurs, dont la majorité peuvent être corrigés en peu de temps.\FL{la question qui vient tout de suite: alors pourquoi pas avoir fait ces corrections?} \draft{entre le 20 et le 30 je vais refaire les calculs avec les corrections pour voir qu'est-ce que ça aurait donné dans le cas échéant.}

D'abord, il restait des erreurs dans les représentations sémantiques\FL{si l'erreur est dans l'input alors il faut corriger l'input et refaire le calcul de précision/rappel, c'est pas un problème. ce que tu dois pas faire} qui ont échappé à notre filtrage post-développement. Par exemple, nous avons inséré \texttt{grill\_1} dans une structure qui pointe vers la classe \texttt{cooking-45.3}, mais nous aurions du insérer \texttt{grill\_3}, qui pointe vers la bonne classe selon l'usage de ce verbe dans l'exemple. L'utilisation d'une mauvaise acception d'un verbe mènera inévitablement à l'impossibilité de récupérer le bon patron de régime.

Ensuite, nos dictionnaires contenaient des erreurs, comme une diathèse manquante pour un patron de régime donné ou alors, l'oubli d'une préposition pour la réalisation de surface d'un actant syntaxique. Les classes par défaut ont aussi leur lot de problèmes. Par exemple, nous avons dû créer une classe \texttt{quote} pour les patrons de régime qui sélectionne des propos rapportés (ex.: \form{Helen told Ellen ``leave the room''}). Toutefois, une erreur s'est probablement\FL{tu trouves pas où est l'erreur?} glissée dans l'encodage de cette classe et le système n'a pas pu la réaliser lors de l'arborisation. Nous avons aussi eu un problème avec les classes (comme nom propre, endroit,) qui n'ont pas pu réaliser en surface les prépositions sélectionnées, bien que nous les avions incluses dans le système.\FL{trouves où est l'erreur}

Ensuite, nous avons eu beaucoup de problèmes liés au manque de données dans VerbNet. Notamment, quelques verbes utilisés dans les phrases d'exemples de VerbNet ne figuraient pas dans leur propre dictionnaire. Par le fait même, les patrons de régime associés à ces verbes ne sont pas connus, ce qui mène à l'impossibilité de générer l'arbre attendu. C'était le cas par exemple de la phrase \form{He begged her to do it}, où le verbe \lex{DO} manquait à l'appel. Ce type de situation était fréquent dans les phrases d'exemples qui comportaient deux verbes, où le verbe subordonné était absent du dictionnaire. Dans d'autres cas, c'était plutôt un des patrons de régime du verbe qui manquait dans VerbNet. Par exemple, nous avions la phrase \form{He backed out of going on the trip}, mais aucune des acceptions de \lex{go} ne contenait le régime nécessaire pour réaliser cette phrase.

Certains problèmes de rappel provenaient d'incompatibilités d'analyse linguistique entre VerbNet et la \ac{TST}. En effet, nous avons représenté chaque phrase exemple en graphe sémantique à la \cite{mel2012semantics}. Comme les patrons de régime font le pont entre la sémantique et la syntaxe, s'il y a une incompatibilité entre la sémantique de l'énoncé et sa représentation syntaxique selon VerbNet, ça mènera à un échec. Par exemple, la phrase \form{The plays and ballets alternate} se traduit par un seul actant syntaxique selon VerbNet. Toutefois, nous considérons qu'il y a deux actants sémantiques, donc deux actants syntaxiques puisqu'ils sont réalisés en surface.\FL{argument en faveur de cette analyse: plays alternate with ballets} Comme l'input sémantique que nous avons fourni au système contenait deux actants sémantiques, le \ac{GP} qui régit la construction (\texttt{NP\_V}) ne peut pas faire le pont entre notre interprétation sémantique et les constructions fournies par VerbNet. 

Nous avons relevé d'autres cas similaires d'incompatibilité sémantique, comme la phrase \form{Gloria slept the sleep of the dead}. Dans cet exemple, \form{the sleep of the dead} est une expression figée qui représente un degré d'intensité (ce qui correspond sémantiquement à la fonction \lexfn{Magn}, cf.~chapitre~\ref{chapgendr}). Ainsi, cette expression est plutôt un prédicat qui modifie \sem{sleep}. Toutefois, VerbNet considère que \lex{sleep} sélectionne l'objet direct \emph{of the dead} et qu'il y a deux actants sémantiques.

\textbf{Mécanisme d'héritage defectueux}

Le dernier problème qui a le plus affecté le rappel est la défectuosité du mécanisme d'héritage des \acp{GP} présenté au chapitre~\ref{ch:implementation}. Concrètement, c'est ce qui permet en théorie qu'une sous-classe, comme \texttt{throw-17.1-1}, hérite des \acp{GP} encodés dans sa classe mère, \texttt{throw-17.1}, sans qu'on ait à les expliciter dans la sous-classe même. Ce mécanisme d'héritage est utilisé abondamment dans GenDR. D'ailleurs, dans l'ancienne version de ce système, les classes verbales étaient aussi organisées de façon hiérarchiques (mais de façon plus rudimentaires). Cependant, il semblerait que ce mécanisme ne fonctionne pas parfaitement.

Tel que présenté au chapitre précédent, dans la nouvelle version de GenDR, les unités lexicales héritent d'une classe verbale, puis cette classe verbale peut hériter d'une autre classe. Les classes verbales mères de VerbNet héritent d'une classe générale \texttt{verb}, dont elle hérite les traits \texttt{dpos=V} et \texttt{spos=verb}. Selon notre évaluation, l'héritage de ce trait fonctionne au moins partiellement puisque l'unité lexicalisée contient en syntaxe de surface le bon trait \texttt{spos=verb}, hérité de la classe générale.

\draft{
Le transfert s'est donc correctement appliqué, mais les patrons de régime ne se sont pas transmis entre classes dominantes et classes dominées. Il semblerait que les traits \texttt{id} et \texttt{dia} qui sont encodés dans le trait \texttt{gp} n'ont pas pu se rendre puisque ceux-ci sont encodés à un niveau plus profond que le trait \texttt{dpos=V} par exemple. Autrement dit, le système peut permettre l'héritage d'un trait de premier niveau, mais est incapable de transmetter un trait enchâssé dans un trait. Malheureusement, nous avions construit notre architecture de cette manière, mais nous pourrions assez facilement réparer ce problème.

Pour pallier à ce mécanisme d'héritage defectueux, nous pourrions directement implémenter, avec nos scripts Python, chaque \ac{GP} de chaque classe mère à chaque classe fille. De cette manière, on garde l'architecture pensée de VerbNet où les classes héritent des patrons des autres classes et règle le problème. Cependant, le défaut de cette solution est que notre dictionnaire sera beaucoup plus lourd puisqu'il existe énormément de sous-classe.

Pour visualiser l'héritage dont nous parlons, voici une courte mise en situation qui nous a été révélée lors de l'évaluation. La phrase \form{Steved tossed the ball from the corner to the garden.} n'a pas pu être rappelée puisque nous n'avions pas le régime nécessaire dans l'entrée du verbe. Le lexème{toss\_3} pointait vers la classe \texttt{throw-17.1-1} qui est une classe dominée \texttt{throw-17.1} qui est elle-même dominée par \texttt{throw-17.1}. Cependant le \ac{GP} dont nous avions besoin pour réaliser la phrase souhaitée se trouvait dans le régime de la classe mère \texttt{throw-17.1}. GenDR n'a donc pas été capable de récupérer les traits nécessaires à l'arborisation, mais il a été capable de réaliser la racine de l'arbre grâce à l'héritage du trait \texttt{dpos=V} encodé dans la classe par défaut \texttt{verb}.

}\FL{shit. c'est prace que tu redéfinis le trait "gp" dans chaque sous-classe, ce qui bloque l'héritage. c'est un défaut de design majeur qu'on aurait dû voir beaucoup plus tôt}

\section{Précision}

\draft{blah}
\[\frac{\text{nombre de structures correctes}}{\text{nombre de structures générées}}\]

L'évaluation de la précision nous donne un taux de 77\,\%,\FL{chiffres bruts} ce qui est un niveau acceptable selon nous.\FL{je trouve ça plutôt bas: si le quart de l'output est incorrect, c'est pas utilisable}

\draft{Les erreurs humaines affectant le rappel ont généralement un impact sur la précision. Par exemple, en mettant la mauvaise acception d'un vocable dans l'input, le système récupère les \acp{GP} de l'acception qui lui est fournie, alors il y a de fortes chances qu'un \ac{GP} matche la diathèse, mais que certaines de ses propriétés syntaxiques mèneront à une arbre de surface agrammatical. Par exemple, le fait d'avoir mis l'acception \sem{grill\_1} dans l'input au lieu de \sem{grill\_3} a généré les arbres suivants: \form{She grilled the steak on me} et \form{She grilled the steak in me}.}\FL{on corrige l'input et on recommence}

Les erreurs importées de VerbNet affectent aussi la précision. Lorsqu'un verbe utilisé dans l'exemple n'est pas répertorié par VerbNet, le système utilise ses règles de lexicalisation  de secours, qui traite un lexème comme un nom quand il n'est pas dans le dictionnaire, ce qui nous donne par exemple la phrase \ungr\form{He begged her for the do}.\FL{faudrait avoir qq part une liste numérotée des phrases de test, pour y faire référence directement} Nous avons relevé d'autres cas similaires dans notre évaluation.

Parmi les informations lexicales extraites de VerbNet, certaines nous ont aussi fait réaliser des phrases quasi-correctes mais bizarres. Cela est une conséquence des \acp{GP} qui permettent de deux à quatre prépositions différentes pour la réalisation d'un même actant. Le système va donc générer plusieurs arbres différents en fonction des prépositions possibles, mais généralement seule l'une d'entre elles donne une phrase grammaticale. Ainsi, en appliquant les\acp{GP} de VerbNet, GenDR génère les phrases \form{The doctor cured pat of pneumonia} et \ungr\form{The doctor cured pat out of pneumonia}.

Un autre type d'erreur de précision provient de l'arborisation dans GenDR. Tel que nous l'avons vu plus tôt, la règle \emph{actant\_gp\_selection} crée autant de racines qu'il y a de \acp{GP} selon la classe verbale utilisée. Ensuite, GenDR sélectionne les \acp{GP} et vérifie que la diathèse de ceux-ci en permet l'application. Il peut alors sélectionner un \ac{GP} fautif,\FL{fautif dans quel sens?} mais qui respecte quand même la diathèse et les contraintes de l'input. C'est ainsi que le système a pu générer la phrase \ungr\form{The street gushed}, puisque rien dans notre système nous empêchait de réaliser cette forme fautive.\FL{à partir de quel input? je comprends pas bien le problème} Pour régler ce problème, il faudrait revoir l'application\FL{conception?} de nos règles.

\section{Synthèse}

\textbf{Rappel:}
GenDR : mécanisme d’héritage defectueux
Erreurs humaines: oubli préposition dans gpcon, erreur d’encodage de diathèse, mauvaise désambiguisation d’un verbe,
VerbNet: pas l’entrée de DO, mauvais gp de believe, manque un gp à go\_2
Incompatibalilité sem-synt: PLays and ballets alternate

\textbf{Précision:}
GEnDR: quand il ne sait pas quoi faire avec une entrée : le considère entre autre comme un nom, et ça donne n’importe quoi, préposiitons bizarres,  utilisation du gp de GO qui donne des trucs fautifs (généralement le cas quand y’a deux verbes), mauvaise désambiguisation de VerbNet qui mène à des emplois de gps fautifs
VerbNet : mauvaise désambiguisation, 


\begin{center}
 \begin{tabular}{||c c c c c c||} 
 \hline
  & DSynt & SSynt & GenDR & Interface Sem-Synt & VerbNet \\ [0.5ex] 
 \hline\hline 
 \hline
 Rappel & 72\%  (73/125) & 62\% (90/125) & 10 & 2 & 5 \\
 \hline
 Précision & 80\% (79/99) & 66\% (66/100) & 8 & 2 & 4 \\ [1ex] 
 \hline
\end{tabular}
\end{center}

Juste les erreurs de GenDR : 
\textbf{précision}: Les prépositions donnent des phrases bizarres. 
\textbf{rappel}: Le mécanisme d'héritage est defectueux.

\begin{center}
 \begin{tabular}{||c c c c||} 
 \hline
  & DSynt & SSynt & GenDR \\ [0.5ex] 
 \hline\hline 
 \hline
 Rappel & 80\%(103/128) & 77\% (99/128) & 9 \\
 \hline
 Précision & 91\% (78/86) & 88\% (76/86) & 6 \\ [1ex] 
 \hline
\end{tabular}
\end{center}

\draft{tableau, discussion des résultats}