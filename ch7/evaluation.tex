%!TEX root = ../memoire.tex

\chapter{Évaluation}\label{ch:eval}

\cite{ReiterInvestigationValidityMetrics2009} expliquent qu'il existe trois types classiques d'évaluation en \ac{GAT}: l'évaluation basée sur l'exécution d'une tâche, l'évaluation automatique, puis l'évaluation humaine. Nous ferons un bref survol de ces trois types pour justifier nos choix.

La méthode d'évaluation basée sur l'exécution de tâches consiste à trouver des évaluateurs qui exécuteront une tâche donnée en se basant sur des rapports rédigés automatiquement. Ainsi, il faut trouver des participants qui seront prêts à donner de leur temps pour exécuter la tâche donnée. \cite{ReiterInvestigationValidityMetrics2009} estiment qu'il s'agit de la méthode qui évalue le mieux le contenu des réalisations d'un système de \ac{GAT} puisqu'on mesure concrètement si le texte généré est de bonne qualité. Par exemple, un texte bien composé permettra aux participants de compléter la tâche rapidement, tandis qu'un texte ilisible ou très mal écrit nuira grandement à la rapidité d'exécution de la tâche et mènera potentiellement à des erreurs. Toutefois comme les auteurs le rappellent, c'est la méthode la plus coûteuse en termes de temps et de ressources puisqu'il faut trouver des participants prêts à faire cette évaluation et il faut préparer une tâche en conséquence.

Parmi les méthodes automatiques, la méthode BLEU, qui a été conçue à la base pour les systèmes de traduction automatique, est relativement commune en \ac{GAT}. Par exemple, \cite{Langkilde-gearyForestbasedstatisticalsentence2000} et \cite{Habash2003MatadorAL} ont utilisé cette méthode pour évaluer leurs systèmes. Elle consiste à comparer le résultat à une phrase étalon préparée manuellement (ou tirée d'un corpus). Toutefois, cette méthode d'évaluation est souvent critiquée. Par exemple \cite{DBLP:conf/semeval/MilleCBW17} soulignent que BLEU évalue bien la couverture, mais ils critiquent sa valeur dans l'évaluation de la qualité des outputs. Dans cet article, ils argumentent que leur réalisateur, FORGe, obtient un score au-dessus de la moyenne pour l'évaluation humaine, mais un score plus faible avec BLEU, et expliquent ce décalage par le fait que leur réalisateur favorise la précision par rapport au rappel.

L'évaluation humaine consiste à noter les phrases produites selons divers paramètres. Par exemple, \cite{BelzFirstSurfaceRealisation2011} proposent comme critères d'évaluation la clarté (qu'on juge en fonction de la facilité à lire le texte), la lisibilité (qu'on juge par la fluidité du texte: construction syntaxique étranges, erreurs grammaticales, etc.) et la similarité de sens (qui teste le paraphrasage). Cette méthode d'évaluation est plus simple à mettre en place que la méthode à base de tâches, mais elle demande plus de temps à exécuter que la méthode automatique.

Pour évaluer GenDR, nous comparerons les outputs de notre système avec les phrases du corpus de VerbNet pour vérifier si nous retrouvons exactement l'arbre syntaxique de surface correspondant à la phrase originale. Ensuite, comme GenDR incorpore un système de paraphrasage puissant, les phrases fournies en input produisent généralement plusieurs outputs à la fois. Donc, nous évaluerons chaque phrase générée, peu importe si on l'attendait ou pas, pour en vérifier la grammaticalité.

\section{Méthodologie d'évaluation}

Pour faciliter l'encodage des graphes sémantiques qui serviront d'input à nos tests, nous avons créé des scripts Python qui nous ont permis d'extraire les phrases exemples de VerbNet et de les intégrer à des fichiers \emph{.str}. Nous avons ainsi obtenu 978 structures vides d'information sémantique, mais contenant les phrases à encoder afin d'accélérer le processus d'encodage de celles-ci. Nous devions ensuite encoder manuellement les graphes sémantiques pour qu'ils représentent les phrases que nous tenterons de générer. \draft{Trois annotateurs se sont divisé la tâche, et les structures ont toutes été contre-validées par un autre annotateur.}

Nous étions maintenant prêts à procéder à l'évaluation. Toutefois, comme nous étions limité dans le temps, nous avons pris 75 structures aléatoirement, dont 25 ont servi lors d'une phase de développement précédant la phase d'évaluation à proprement parler.

La phase de développement nous a permis de constater que certains noms d'entrées lexicales, appartenant à des parties du discours différentes, apparaissaient en double dans notre dictionnaire. Par exemple, les entrées pour le verbe \lex{work} et le nom \lex{work} avaient le même nom, donc le système n'accédait qu'à la dernière des entrées et ignorait la première. Nous avons conséquemment procédé à un filtrage pour remédier à la situation en donnant un nom différent aux entrées homonymiques. Ensuite, après la phase de développement terminée, nous avons procédé à l'évaluation sur les 50 structures restantes.
                              
\subsection{Rappel}
Nous avons mesuré le rappel en calculant le ratio entre le nombre de structures attendues et le nombre effectivement généré. Ainsi, chaque structure manquante à l'appel est considérée comme une erreur et le calcul se fait sur 50 puisque nous avons testé sur 50 phrases provenant du corpus de VerbNet. Les résultats sont présentés au tableau~\ref{fig:evaluationrappel}.
\[\text{Rappel} = \frac{\text{nombre de structures attendues générées}}{\text{nombre de structures attendues}}\]

\begin{table}
\caption{Évaluation du rappel}
\label{fig:evaluationrappel}
\begin{tabular}{lrr}
 % \hline
 % \multicolumn{3}{|c|}{Rappel} \\
 \toprule
   & RSyntS\\
 \midrule
 Nombre de structures attendues   & 50\\
 Nombre de structures générées &  45\\
 Silences dus à GenDR & 2\\
 Silences dus à VerbNet    & 2\\
 Silences dus à une incompatibilité TST/VerbNet & 1\\
 \midrule
Pourcentage global & 90\% \\
Pourcentage GenDR & 100\% \\
\bottomrule
\end{tabular}
\end{table}

Les \textbf{silences dûs à VerbNet} proviennent de deux problèmes distincts. D'abord, c'est à cause d'un patron de régime non existant pour la classe verbale \texttt{"escape-51.1"} qui correspond à la classe verbale attitrée au lexème \lex{go\_2}. La structure 974, qui représente la phrase \form{He backed out of going on the trip} contient deux verbes: \lex{back out} et \lex{go\_2}, mais VerbNet n'a pas le patron de régime nécessaire au second verbe pour pouvoir réaliser \form{$X$ goes on a trip}. Ainsi, GenDR était dans l'impossibilité de réaliser la structure attendue puisqu'il nous manquait des informations sur la combinatoire de \lex{go\_2}. Il nous rendait des arbres de surface incomplets s'arrêtant à la lexicalisation de \lex{go\_2}, sans actants. Puis, le deuxième problème hérité de VerbNet concerne la structure 267 représentant la phrase \form{He converted to believing in Buddha}. L'erreur vient du fait qu'il n'existe aucun patron de régime dans VerbNet pour les unités lexicales \lex{believe\_1}, \lex{believe\_2} ou \lex{believe\_3} qui puisse réaliser la forme \form{$X$ believes in $Y$}, ce qui fait que la phrase ne peut pas être générée.

\textbf{Le silence dû à une incompatibilité} entre la \ac{TST} et VerbNet provient de la modélisation sémantique de certains phénomènes langagiers par la \ac{TST}. Une seule structure n'a pas pu être générée: 630--\form{She laughed in embarrassment}. Ainsi, l'incompatibilité est due au fait que nous avons représenté ces phrases en graphes sémantiques à la \cite{mel2012semantics}. 
Pour la structure 630, l'incompatiblité provient du fait que \form{in embarrassment} est sémantiquement un modificateur du sens de \sem{laugh}. On l'a donc représenté ainsi dans notre graphe sémantique. Toutefois, VerbNet considère que \lex{embarassment} correspond au deuxième actant syntaxique du verbe, et que c'est un objet sélectionné par celui-ci, qui requiert une préposition lors de sa réalisation de surface. Ils ont donc créé un patron de régime pour tenir compte de cette construction, mais ce \ac{GP} est inutile selon nous, puisque \lex{embarassment} n'est pas sélectionné par \lex{laugh} selon nous. Pour cette raison nous ne l'avons pas modélisé dans le graphe d'input comme étant un actant sémantique de \sem{laugh} mais plutôt comme un modificateur de celui-ci, car c'est ce que le sémantème \sem{embarassing} fait. Puisque nous n'avons pas encodé de règles de correspondance permettant de réaliser la forme \form{in embarassment} en tant que modificateur, notre système n'a pas pu générer la phrase attendue. En revanche, nous avons généré \form{She laughed embarassingly}, qui est une paraphrase de la phrase attendue et que notre système permettait de réaliser \draft{c'est une fonction lexicale Adv1}. 

\textbf{Les silences dus à GenDR} proviennent du fait que GenDR ne gère pas la coordination qui permet de créer un actant syntaxique à partir de deux actants sémantiques. Les phrases que nous n'avons pas pu générer à cause de cela sont 036--\form{Plays and ballets alternate} et 330--\form{This flyer and that flyer differ}. En effet, nous n'avons pas de mécanisme dans GenDR pouvant recréer ce type de structure en surface, donc le fait d'encoder \sem{play} et \sem{ballet} comme deux actants sélectionnés par le verbe menait nécessairement à un échec de réalisation. Par contre, nous avons pu générer \form{Plays alternate with ballets} et \form{This flyer differs from that flyer}, qui sont des paraphrases des phrases attendues. Cela démontre par le fait même la capacité de paraphrasage de GenDR.

Finalement, en ce qui concerne l'évaluation du rappel, nous n'avons pas comptabilisé comme une erreur l'impossibilité de générer des compléments du nom comme \form{oil's price}. GenDR ne permettait pas de réaliser ce type de forme et c'est un problème dont nous avons hérité, mais ce n'était pas l'objet de notre recherche, puisque nous cherchions à intégrer correctement les patrons de régime des verbes, alors que ce type de construction possessive relève des patrons de régime des noms. Nous les avons néanmoins notés dans l’annexe pour montrer ce que nous attendions comme sortie (section~\ref{ch:annexe}).

\subsection{Précision}\label{sec:precision}
Pour mesurer la précision, nous avons mesuré le ratio entre le nombre de structures grammaticalement correctes générées et le nombre total de structures générées:
\[\text{Précision} = \frac{\text{nombre de structures correctes}}{\text{nombre de structures générées}}\]

\begin{table}
\caption{Évaluation de la précision}
\label{fig:evaluationprecision}
\begin{tabular}{lrr}
 \toprule
  & DSynt & SSynt\\
 \midrule
 Nombre de structures générées   & 106  & 116 \\
 Nombre de structures correctes  &  90  & 93   \\
 Erreurs dues à GenDR & 0 & 0\\
 Erreurs dues à VerbNet    & 16 & 23\\
 Erreurs dues à une incompatibilité TST/VerbNet & 0 & 0\\
 \midrule
 Pourcentage global & 85\%  & 80\% \\
 Pourcentage GenDR & 100\%  & 100\% \\
 \bottomrule
\end{tabular}
\end{table}

Pour l'évaluation de la précision (voir la tableau~\ref{fig:evaluationprecision}), nous avons séparé les réalisations de la \ac{RSyntP} de celles de la \ac{RSyntS} car les résultats différaient concernant les erreurs dues à VerbNet. Cet écart provient du fait qu'il existe certaines \textbf{prépositions régies par le verbe} qui ne sont pas encore réalisées en syntaxe profonde, mais que lorsqu'elles sont réalisées en syntaxe de surface, elles \textbf{mènent à des incohérences}. Cela est une conséquence des \acp{GP} qui permettent à certains de leurs actants de se réaliser par plus d'une préposition. Le système génèrera alors autant d'arbres différents qu'il y a de choix de prépositions pour un actant donné. Mais très souvent, seule l'une d'entre elles produit une phrase grammaticale. Par exemple, en fournissant en input la structure 177, GenDR génère les phrases \form{The doctor cured pat of pneumonia} et \ungr\form{The doctor cured pat out of pneumonia}. La deuxième étant incohérente à cause de la présence de la préposition \lex{out of} qui rend la phrase agrammaticale. Nous constatons que sept erreurs en surface sont dues à ce phénomène.

Les erreurs que nous vous présenterons maintenant concernent les deux niveaux de représentations (\ac{RSem}--\ac{RSyntP} et \ac{RSyntP}--\ac{RSyntS}). D'abord, un type d'erreur courant de VerbNet provient de la \textbf{sélection d'un \ac{GP}} qui respecte les contraintes imposées par la structure sémantique (les actants sémantiques), mais dont \textbf{le produit génère une phrase agrammaticale}. Par exemple, l'input de la structure 968--\form{Barry Cryer erased at the writing} comprenait les actants sémantiques \texttt{1} et \texttt{2}, ce qui a permis au patron de régime \texttt{NP\_V\_PP\_at\_Conative} de s'appliquer, générant la forme \ungr\form{Barry Cryer erased at the writing}. En réalité, le type de phrase que ce \ac{GP} illustre normalement pour la classe verbale \texttt{"wipe\_manner-10.4.1-1"} est \form{Brian wiped at the counter}. De cette façon, la présence du \ac{GP} (\texttt{NP\_V\_PP\_at\_Conative}), dont la diathèse correspond aux actants sémantiques demandés par l'input, a permis au \ac{GP} d'être sélectionné et de réaliser un arbre syntaxique profond agrammatical.

Finalement, le dernier type d'erreur rencontré dans l'échantillon provient de \textbf{l'absence d'un verbe}. En effet, le verbe \lex{do} ne figure pas dans VerbNet. Cela n'a pas été un problème pour le rappel puisque GenDR est doté de règles de secours permettant de traiter des sémantèmes qu'il ne connaît pas. Ainsi, dans la phrase \form{I begged her to do it}, on a pu générer la phrase attendue car GenDR a opéré diverses réalisations dont, une correspondait à celle que nous attendions. Le système a supposé que c'était un verbe puisque le patron de régime de \lex{beg} avait mis comme contrainte sur le n\oe{}ud du second actant syntaxique que ce soit un verbe. Donc, il assume que c'est bel et bien un verbe et il récupère un patron de régime assigné par défaut aux verbes, soit \texttt{NP\_V\_NP} (un patron de verbe transitif direct), ce qui permet à \lex{do} de sélectionner les deux actants sémantiques \sem{me} et \sem{it}, le tout résultant en une bonne réalisation. Mais pour tenir compte de toutes les possibilités, GenDR suppose ensuite que c'est possiblement un nom (\texttt{dpos=N}), et cela crée un nouvel arbre profond où \lex{do} est un nom. Le résultat de cette supposition erronnée est \ungr\form{I begged her for the do}. Nous n'avons relevé que très peu de scénarios similaires, mais ils étaient tous liés au fait que \lex{do} ne figure pas parmi les verbes répertoriés par VerbNet. Cela est probablement dû au fait que \lex{do} est généralement utilisé comme auxiliaire en anglais, donc ils l'ont exclu de leur ressource, tout comme ils ont exclu les verbes modaux.

\section{Synthèse}

L'évaluation du rappel démontre que l'importation de VerbNet dans GenDR a été un succès (90\%) et que la couverture de GenDR est dorénavant beaucoup plus grande qu'elle l'était (6\,393 verbes, 274 classes verbales et 278 patrons de régime).

L'évaluation de la précision nous montre qu'il y a quelques ajustements à faire puisque l'encodage des informations dans VerbNet mène à un taux d'erreur faible, mais non-négligeable (15--20\%). La raison pour laquelle les réalisateurs profonds à base de règles sont encore utiles est leur capacité à produire des phrases grammaticales, toutefois, nous démontrons ici qu'une partie des phrases réalisées par notre système sont incorrectes. Cependant, ces erreurs proviennent de VerbNet, donc notre réalisateur fonctionne bien, il n'a fait qu'appliquer les règles de grammaires et récupérer les informations dans ses dictionnaires. Pour remédier à la situation, nous pourrions corroborer les patrons de régime encodés dans VerbNet avec les patrons de régime de verbes encodés dans d'autres ressources similaires. De cette manière nous pourrions adapter VerbNet pour qu'il n'engendre pas de structures agrammaticales. Bref, nous pensons qu'il est possible d'améliorer la précision de l'implémentation de VerbNet, mais cela demandera du temps et des ressources pour analyser les patrons potentiellement problématiques et retirer ou ajouter les patrons de régime nécessaires dans les classes verbales.

