%!TEX root = ../memoire.tex
\chapter{Évaluation}\label{ch:eval}

Avant d'entrer dans le vif du sujet, il serait pertinent de faire un bref retour sur les méthodes d'évaluation en \ac{GAT}. \cite{ReiterInvestigationValidityMetrics2009} expliquent qu'il y existe trois types classiques d'évaluation. Ils nomment d'abord la méthode d'évaluation qui se base sur l'exécution d'une tâche en utilisant les textes génénés automatiquement. Ils nomment aussi la méthode d'évaluation humaine. Et finalement, ils parlent des méthodes métriques (automatiques). \cite{ReiterBuildingNaturalLanguage2000} s'étant penché plus tôt sur la question de la validité des méthodes d'évaluation automatiques, ils étaient en faveur d'une évaluation faite par des humains. Toutefois, environ une décennie plus tard, \cite{ReiterInvestigationValidityMetrics2009} remarquaient que les méthodes d'évaluation automatiques se faisaient de plus en plus populaires. Notamment, la méthode BLEU qui avait été développée, à la base, pour les systèmes de traductions automatiques. Nous ferons donc un bref survol de ces méthodes pour décider laquelle se prête le mieux à notre expérience.

BLEU a été créé à la base pour évaluer les rendements des traductions automatiques. Il s'agissait de comparer des outputs d'un système de traduction automatique à un ensemble de traductions humaine (servant de point de référence). Comme la traduction automatique et la génération automatique comportent toutes les deux l'aspect automatique, des chercheurs comme (Langkilde 2002; Habash 2004) ont estimé que la \ac{GAT} bénéficierait de cette méthode d'évaluation. Toutefois, lors de son passage à SemEval2017, FORGe a notamment été évalué par des méthodes métriques et \cite{DBLP:conf/semeval/MilleCBW17} ont fait une brève critique de cette méthode. Ils soulignent que BLEU évalue effectivement bien la couverture, mais comporte des lacunes pour analyser la qualité de chaque output. Leur système avait un score au dessus de la moyenne pour ce qui était de l'évaluation humaine, mais avait reçu un score plus faible pour selon la méthode BLEU. Ils expliquent ce décalage en mentionnant que FORGe mettait de l'avant la qualité de ses outputs par rapport à la quantité. De sorte que ce système filtre à deux reprises les constructions fautives ou potentiellement fautives. (Scoot et Moore, 2007) donnaient aussi quelques mises en garde de la méthode BLEU en précisant qu'elle n'évalue pas toujours correctement des propriétés linguistiques cruciales.

Les méthodes d'évaluation basées sur l'exécution d'une tâche à l'aide de textes générés automatiquement sont assez courantes selon \cite{ReiterInvestigationValidityMetrics2009}. Ces auteurs estiment qu'il s'agit de la méthode qui évalue le mieux le contenu des réalisations d'un système de \ac{GAT}. Toutefois, ils nous mettent en garde que c'est malheureusement la méthode la plus coûteuse en termes de temps et de ressources. En résumé, plus l'output est lisible et clair, plus hautes sont les chances que la tâche soit réalisée rapidement et correctement. Cette méthode n'est pas toujours facile à mettre en place car il faut trouver des participants prêt à donner de leur temps pour lire les rapports générés et effectuer une tâche correspondate. 

Finalement, il y a l'évaluation humaine, plus simple à faire que la méthode à base de tâche, mais plus lente que la méthode automatique. Toutefois, cela reste une méthode très populaire dans le domaine. Il s'agit de coter les outputs en fonction de leur performance à produire des phrases syntaxiquement et sémantiquement acceptables au bon jugement d'un évaluateur.

Considérant ces trois méthodes, nous devons en exclure deux: celle qui est basée sur une tâche et la méthode automatique. D'abord notre système ne réalise pas du texte dans un but précis. On n'a pas de tâche à effectuer pour tester la validité des réalisations. De plus, nous n'avions ni le temps, ni les ressources pour entreprendre ce type d'évaluation. Ensuite, nous ne pouvons pas utiliser la méthode automatique car notre système génère des arbres de dépendances de surface. Les systèmes utilisant la méthode automatique comparent des chaînes de caractères (des réalisations de surface où les textes sont linéarisées et morphologisés). Il nous est donc impossible de comparer nos arbres syntaxiques de surface avec du texte. Il ne reste qu'une méthode d'évaluation s'offrant à nous et c'est celle faite par des humains basée sur nos jugements.

\section{Mise en place de l'évaluation}
Pour procéder à l'évaluation de notre système, nous avons utilisé les outputs du script \ref{structurepython} (voir le chapitre \ref{ch:python}). Ceux-ci étaient des structures sémantiques vides dépourvues de prédicats et d'arguments. Il n'y avait que le code pour encadrer le graphe et le texte à reproduire sémantiquement. Nous avons comblé les 978 structures vides en y encodant les unités et relations sémantiques qui correspondaient à l'énoncé. La tâche est simple, nous passerons ces structures sémantiques en input à notre système et nous évaluerons les réalisations produites.

Comme nous avions une quantité limité de temps, nous avons décidé de prendre un échantillon des 978 strucutres sémantiques. Nous en avons choisi 75 aléatoirement. Parmi celles-ci, 25 ont servi à une partie développement précédent la phase d'évaluation comme telle. Ces 25 structures ont été passées au système afin de voir quels sont les problèmes immédiats que nous pouvons réglés sans vérifier si la qualité des arbres produits. Cette phase de développement nous a permi de constater qu'une bon nombre de nos inputs comportaient des problèmes. Nous avons donc noté le type de problème que les inputs comportaient afin de corriger le tir pour la partie évaluation. La partie de développement nous a aussi permi de constater que certains lexèmes appartenant à des PDD différentes apparaissaient en double dans notre dictionnaire. Autrement dit, le verbe \lex{work} et le nom \lex{work} ont la même forme, le système ne sait pas comment les différencier. Cela a donc une incidence sur la réalisation puisque le système construit l'arbre syntaxique à partir du premier lexème qu'il récupère. Donc si nous avions besoin du verbe dans l'arbre et que le système choisi le nom, la réalisation échouera nécessairement. C'est pourquoi nous avons procédé à un filtrage massif de ces cas. Nous les avons réglé en créant une entrée sémantique dans le semanticon qui contiendra les deux entrées lexicales: une version verbale et une version nominale. Elles seront distinguées ainsi dans le semanticon: \lstinline!work { lex = work_n  lex= work_2}!. Bref, nous n'avons pas analysé en profondeur la nature des générations à cette étape, nous voulions seulement répertorier et corriger les problèmes de surface.

Ensuite, nous avons passé au peigne fin chacun des inputs qui seront évalués. Nous nous sommes assurés que les problèmes d'entrées lexicales répertoriés dans la partie développement étaient corrigés en vue de l'évaluation. Finalement, nous avons pu procéder à la phase de tests. Nous avons testé les 50 structure sémantiques restantes. Pour ce faire, nous avons développé un script qui générait automatiquement toutes les représentations visuelles et textuelles des passages RSem-RSyntP et RSyntP-RSyntS. La partie visuelle permettait de regarder les différentes constructions d'arbres rapidement pour voir lesquelles étaient des réussites ou des échecs. La partie textuelle nous permettait de voir les informations sur les n\oe{}uds. Par exemple, quel était le patron de régime sélectionné pour cette arborisation ou alors, quelle était la diathèse sélectionnée, ou la partie du discours demandée par le n\oe{}ud, etc. Ces informations ne sont pas explicitées dans le format graphique de présentation des outputs. 
                              
\section{Rappel}

Nous avons évalué le rappel ainsi:\[\frac{\text{nombre de structures attendues générées}}{\text{nombre de structures attendues}}\]. Cette méthode d'évaluation produit un rappel de 62\%. Ce score se situe au-dessous de nos attentes. Mais il est expliqué par plusieurs facteurs cruciaux dont la majorité peut être corrigée en peu de temps. \draft{entre le 20 et le 30 je vais refaire les calculs avec les corrections pour voir qu'est-ce que ça aurait donné dans le cas échéant.}

D'abord, il y a les erreurs d'encodage qui ont su échapper aux mailles du filet lors de notre filtrage post-développement. Il y avait très peu d'erreur dans l'input sémantique. Nous en avons relevé un seule. Il s'agit de l'emploi d'une mauvaise forme d'un verbe. Nous avons pris grill\_1 et il aurait fallu prendre grill\_3. L'utilisation d'une mauvaise acception d'un verbe mène à l'impossibilité de récupérer le bon patron de régime. En termes d'erreurs humaines, certaines se sont glissées dans les dictionnaires à notre insu. Notamment, une diathèse manquante pour un patron de régime donné. Cela mène à l'impossibilité de réaliser le gp. Ou alors, l'oubli d'une préposition dans une actant syntaxique. Cela fait en sorte qu'en surface, la phrase n'est pas celle qu'on attendait.

Les classes par défaut ont aussi leur lot de problèmes. Effectivement, nous avons dû créer une classe \texttt{quote} pour les patrons de régime qui sélectionne des paroles (Ex: Helen told Ellen "leave the room"). Toutefois, une erreur s'est probablement glissée dans l'encodage de cette classe et le système n'a pas pu la réaliser lors de l'arborisation. Nous avons aussi eu un problème avec les classes qui sélectionnent certaines prépositions. Le système n'est pas capable de récupérer cette préposition dans la classe par défaut, ce qui empêché de réaliser la phrase souhaitée. 

Nous avons eu beacoup de problèmes de rappel lié aux manque de données de la part de VerbNet. Effectivement, il y eut quelques cas de verbes utilisés dans les phrases exemples de VerbNet qui ne figuraient pas dans leur propre dictionnaire verbal. Par le fait même, les patrons de régime associés à ces verbes ne sont pas encodés. Cela mène à l'impossibilité de générer l'arbre qu'on se serait attendu de voir. beg, believe. C'était souvent le cas lorsque la phrase exemple contenait deux verbes. Comme on n'a pas les informations requises pour process le second verbe (diathèse et gp) l'arbre que nous pensions généré ne sera jamais retrieve par GenDR. 

De plus, parmi les problèmes qui nous sont hérités de VerbNet, il y aussi eu quelques cas où la structure sémantique de l'input demandait un patron de régime Y de la part du prédicat, mais que ce patron de régime ne figurait pas parmi la liste de gps de la classe associée à ce verbe. Ou bien he backed out of going  (go\_2: n'avait pas le gp pour réalsier on the trip)

Il y aussi des problèmes due à des incompatibilités sémantique-syntaxe entre VerbNet et la TST. Effectivement, nous n'avons pas tenté de créer une struture sémantique pour qu'elle plaise au patron de régime donné par VerbNet. Cela fait en sorte que quelques des tests allaient nécessairement échouer. Par exemple, la phrase \form{Tom jumped the horse over the fence} est régie par le verbe jump\_2 qui est exprimé par le gp suivant: \lstinline!gp = { id=NP_V_NP_PP_SPATIAL_location dia=123 }!.Les trois actants sémantiques étant \sem{Tom} \sem{horse} et \sem{fence} selon VerbNet. Toutefois, nous considérons que dans ce scénario, le verbe \lex{jump} en surface exprimerait plutôt \sem{faire sauter}. Il existe une fonction lexicale permettant ce genre de situation, mais nous ne l'avons pas encodé dans cette version de VerbNet puisque nous ne touchons pas aux fonctions lexicales. Nous avons noté d'autre cas d'incompatiblités théoriques lors de la phase de développement. 

Nous avons relevé beaucoup de cas d'utilisation de verbe support en surface ('X took a flight' ). Le gp utilisé pour take a flight est bon en soi, mais la représentation sémantique de take est flight ne devrait pas faire appel au gp de take, mais plutôt au gp de flight qui encode les fonctions lexicales (verbe support). et on ne perdrait pas le sens. le \sem{fly} incorpore les lexicalisations \lex{fly} et  \lex{flight}. Took est réalisé comme verbe support en récupérant la fonction lexicale encodée sous \lex{flight}. Nous avons aussi relevé d'autres types de collocations dont X  slept the sleep of the dead. Dans ce cas \form{the sleep of the dead} est une expression figée qui signifie une intensité. Elle modifie le verbe sleep. VerbNet laisse sous-entendre que sleep sélectionne un objet direct qui lui sélectionne un complément du nom. Mais en fait, il ne s'agit que d'un verbe modifié par un intensificateur.

Finalement, le dernier problème qui a le plus affecté le rappel est la défectuosité du mécanisme d'héritage des gps entre les classes dominées et les classes dominantes. Il s'agit du mécanisme dont nous vantions les mérites au début du chapitre. Concrètement, il s'agit d'un moyen qui fait en sorte qu'une class x pointe vers une classe y et hérite des traits encodés dans la classe y. Mais il se trouve que le mécanisme d'héritage des traits fonctionne partiellement. 

Tel que présenté dans le début du chapitre, les unités lexicales pointent vers des classes verbales, une classe verbale peut pointer vers une autre classe verbale, et les classe verbales non-dominées pointent vers la classe par défaut \texttt{verb}. Celle-ci donne les traits dpos=V et spos=verb à tous les lexèmes pointant vers des classes verbales. L'héritage de ce trait est réussi à travers les classes dominées et dominantes car le lexème de surface contient les traits. C'est pourquoi nous disons que le mécanisme fonctionne partiellement. Mais les patrons de régime ne semblent pas se transmettre d'une classe dominante à une classe dominée. Il semblerait que les traits \texttt{id} et \texttt{dia} qui sont encodés dans le trait \texttt{gp} ne sont pas transmis. Ce problème semble provenir de MATE qui n'est pas capable de transmettre l'héritage d'un trait à l'intérieur d'un autre trait. Pour mieux illustrer le concept, nous prendrons le cas de \form{Steved tossed the ball from the corner to the garden.}. Le sémantème \sem{toss\_3} pointait vers la classe \texttt{throw-17.1-1} qui est une classe dominée par "throw-17.1" qui est domineé par "throw-17.1". Cependant le patron de régime dont nous avions besoin pour réaliser la phrase souhaitée se trouvait dans le régime de la classe mère "throw-17.1". GenDR n'a donc pas été capable d'aller chercher les traits id et dia. Mais il a été capable de réaliser la racine de l'arbre avec le lexème \lex{throw\_3} en héritant du trait dpos de la classe par défaut \texttt{verb}.

Pour pallier à ce mécanisme d'héritage deffectueux, nous pourrions directement implémanter tous les gps des classes dominantes dans les classes dominées. Ainsi on garde l'architecture pensée de VerbNet où les classes héritent des patrons des autres classes. Le défaut de cette solution est que notre dictionnaire sera saturé d'information puisqu'il existe énormément de sous-classe. Toutefois, cette solution est facilement adaptable puisque nous avons encore les scripts Python ayant servi à extraire les patrons de régime et à créer le lexicon. Nous n'avons qu'à modifier le script pour que chaque sous-classe hérite explicitement des gps des classe qui les domine. Évidemment, cela aurait aussi un impact direct sur la précision, puisqu'on a plus de chance de générer une erreur si plusieurs gps sont disponibles.

\section{Précision}

Nous avons noté la précision de cette manière:\[\frac{\text{nombre de structures correctes}}{\text{nombre de structures générées}}\]. Cela nous donne une précision de 77\%.

Les erreurs humaines que nous avions mentionnées pour le rappel ont généralement un impact sur la précision. Par exemple, en mettant la mauvaise désambiguisation d'un verbe dans l'input, alors les patrons de régime utilisés pour réaliser la phrase ne seront pas les bons. Cela peut générer de bonnes phrases, si par chance un bon gp s'y trouve. Mais on a vu que cela pouvait générer des phrases incongrues: \form{She grilled the steak on me} ou \form{She grilled the steak in me}.

Les erreurs de VerbNet affectent aussi la précision de l'output négativement. Lorsqu'un verbe utilisé dans l'exemple n'est pas répertorié par VerbNet. Le système utilise ses règles de secours de lexicalisation et va tout de même tenter de réaliser quelque chose. C'est le cas d'une phrase ayant le verbe \lex{do} pour \form{I begged her to do it}. Comme GenDR n'a pas hérité de l'entrée que VerbNet aurait dû avoir, le système a tenté diverses réalisations. Certaines d'entre elles échoueront, mais d'autres seront réalisées en surface. C'est le cas de \form{I begged her for the do.}. Dans ce cas, GenDR a supposé que c'était possiblement un nom. Et comme il existait un patron de régime de \lex{beg} permettant cette construction, alors le système a réalisée cette incongruité. Nous avons relevé d'autres cas similaires dans notre évaluation.

Les informations extraites de VerbNet nous ont aussi fait réaliser des phrases quasi-correctes mais bizarres. C'est une conséquence des patrons de régime qui permettent 2 à 4 prépositions différentes pour la réalisation d'une actant syntaxique en surface. Le système va donc générer deux arbres différents corrects. Chacun d'entre eux aura une préposition différente pour l'actant concerné, mais généralement seule l'une d'entre elle génère une phrase grammaticale. Ainsi, GenDR a généré les phrases \form{The doctor cured pat of pneumonia} et \form{The doctor cured pat out of pneumonia}. La deuxième est un arbre bien construit, mais dont la sémantique est fautive.

Un autre type d'erreur de précision provient de la manière dont GenDR génère les arbres. Tel que nous l'avons vu plus tôt, le système crée x nombres de racines pour un input où x est le nombre de patron de régime inclu pour une classe verbale. 

Ensuite, on sélectionne les gps et on vérifie que la diathèse permet l'appplication du gp sélectionné. C'est là que le problème prend forme. GenDR peut ainsi sélectionner un gp fautif qui respecte la diathèse et les contraintes sur le n\oe{}ud. Mais, il est possible que l'arborisation échoue parce que l'un des actants syntaxiques choisi contient une préposition qui rendra la phrase agrammaticale. Concrètement pour régler ce problème, il faudrait revoir l'application de nos règles. Par exemple la phrase \form{The street gushed}. Rien dans notre système nous empêchait de réaliser cette forme fautive.

\section{Synthèse de l'évaluation}

\draft{Est-ce que c'est pertinent que je parle de la f1 ? Elle est de 69\%}
\draft{Est-ce que j'ai besoin de faire un retour sur ce qui a été dit ? Ou bien j'en parle dans ma conclusion finale}

Ce qu'on retire de tout cela sont deux problèmes majeurs. Le mécanisme d'héritage qui fonctionne partiellement avec l'architecture présente qui nuit au rappel.

De plus, il faut aussi tenir compte du fait que nous avons testé sur 5\% des structures d'input. Mais nous pensons que c'est un 5\% significatif. Les mêmes erreurs revenaient souvent dans l'analyse des résultats.

